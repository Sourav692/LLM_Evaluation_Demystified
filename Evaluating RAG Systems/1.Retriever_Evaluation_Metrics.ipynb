{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40593191-0b62-4413-86cd-8899710603f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reference Link:** [RAG Systems Essentials (Analytics Vidhya)](https://courses.analyticsvidhya.com/courses/take/rag-systems-essentials/lessons/60148017-hands-on-deep-dive-into-rag-evaluation-metrics-generator-metrics-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever Evaluation Metrics\n",
    "\n",
    "## Overview\n",
    "- This notebook demonstrates how to evaluate RAG system retrievers using DeepEval metrics\n",
    "- It focuses on three key evaluation metrics for assessing retrieval quality in RAG pipelines\n",
    "\n",
    "## Key Evaluation Metrics\n",
    "\n",
    "### **Contextual Precision**\n",
    "- **Purpose**: Measures whether relevant document chunks are ranked higher than irrelevant ones\n",
    "- **Input Requirements**: Query, actual output, expected output, and retrieval context\n",
    "- **Scoring**: Evaluates ranking quality of retrieved documents (0.0 to 1.0)\n",
    "- **Use Case**: Assesses how well the retriever prioritizes relevant information\n",
    "\n",
    "### **Contextual Recall**\n",
    "- **Purpose**: Measures how well the retrieval context aligns with expected output\n",
    "- **Input Requirements**: Query, actual output, expected output, and retrieval context\n",
    "- **Scoring**: Evaluates coverage of expected information in retrieved documents\n",
    "- **Use Case**: Determines if retriever captures all necessary information\n",
    "\n",
    "### **Contextual Relevancy**\n",
    "- **Purpose**: Measures overall relevance of retrieved context for a given query\n",
    "- **Input Requirements**: Query, actual output, and retrieval context\n",
    "- **Scoring**: Evaluates general relevance of all retrieved information\n",
    "- **Use Case**: Assesses overall quality of retrieved content\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "- **DeepEval Framework**: Uses DeepEval's LLM-based evaluation metrics\n",
    "- **LLM Judge**: GPT-4o model evaluates relevance and provides reasoning\n",
    "- **Test Cases**: Creates LLMTestCase objects for systematic evaluation\n",
    "- **Thresholds**: Configurable success thresholds (default: 0.5)\n",
    "- **Verbose Mode**: Provides detailed reasoning for metric scores\n",
    "\n",
    "## Evaluation Process\n",
    "\n",
    "1. **Setup**: Run existing RAG pipeline to get retrieval results\n",
    "2. **Context Preparation**: Extract and format retrieved documents\n",
    "3. **Metric Configuration**: Set up evaluation parameters and thresholds\n",
    "4. **Testing**: Run evaluation on test cases with different contexts\n",
    "5. **Analysis**: Review scores, reasons, and pass/fail results\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- **Quality Assurance**: Systematic evaluation of retrieval performance\n",
    "- **Debugging**: Identifies issues with document ranking and relevance\n",
    "- **Optimization**: Provides metrics to improve retriever performance\n",
    "- **Transparency**: Clear reasoning for evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Build_RAG_Pipeline_with_Source.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b0ac04-8452-41c7-8ea0-1bd223c44b03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "MTTk1BaCBfYa"
   },
   "source": [
    "# Retriever Evaluation Metrics\n",
    "\n",
    "![](https://i.imgur.com/5S4FhMB.png)\n",
    "\n",
    "The retrieval process generally includes these steps:\n",
    "\n",
    "- Convert the initial input query into an embedding using an embedding model of your choice (e.g., OpenAI's `text-embedding-3` model).\n",
    "- Conduct a vector search with the embedded input on a vector database that holds your vectorized knowledge base, retrieving the top-K most \"similar\" document chunks.\n",
    "- Optionally user a Reranker to rerank the retrieved results\n",
    "\n",
    "\n",
    "Key Metrics to Evaluate here include:\n",
    "\n",
    "- Contextual Precision\n",
    "- Contextual Recall\n",
    "- Contextual Relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d4b9c03-88eb-480d-8818-3281e1dce5b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "gQOoZNPzDdjY"
   },
   "source": [
    "## Contextual Precision\n",
    "\n",
    "The contextual precision metric measures your RAG pipeline's retriever by evaluating whether document chunks (nodes) in your `retrieval_context` that are relevant to the given `input` are ranked higher than irrelevant ones.\n",
    "\n",
    "`deepeval`'s contextual precision metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score using an LLM as a judge.\n",
    "\n",
    "In `deepeval`, to use the ContextualPrecisionMetric, you'll have to provide the following arguments when creating an `LLMTestCase`:\n",
    "\n",
    "- `input` : Input Query\n",
    "- `actual_output` : Actual LLM Response (not used in the computation)\n",
    "- `expected_output` : Expected LLM Response (ground truth answer)\n",
    "- `retrieval_context` : Top-N retrieved document chunks (nodes) from Vector DB\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/oVwrRAU.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98fee6a9-a9ee-409a-a82f-74bb08c3f4de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1400,
     "status": "ok",
     "timestamp": 1734095838031,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "deG08v9sF0pY",
    "outputId": "8905b41c-2671-4a66-889c-eb7423d3343d"
   },
   "outputs": [],
   "source": [
    "query = \"What is AI?\"\n",
    "response = rag_chain_w_sources.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "509ebdb4-1b67-4ff1-b059-a937f176dcd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FXisPv9hSbSL"
   },
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e27c8d-6a7d-4de7-872e-43bb5049dc89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1734095838362,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "v9cW2lv1IzIx",
    "outputId": "d9fdaa17-172c-4026-971a-dc418ed41c82"
   },
   "outputs": [],
   "source": [
    "retrieved_context = [doc.page_content for doc in response['context']]\n",
    "retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d16b5a-442e-4bbd-8044-ac3fdf463793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1734095840510,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "Bm8St8H7F5nS"
   },
   "outputs": [],
   "source": [
    "human_answer = \"\"\"AI, also known as Artificial Intelligence is used to build complex systems for applications\n",
    "                  like virtual assistants, robotics and autonomous vehicles.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea6a017b-bc10-48df-9779-f05b4f591171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1734095841851,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "nOhNhwphJEfU",
    "outputId": "9504fe9d-340b-4cdd-da6a-71d88d96ee49"
   },
   "outputs": [],
   "source": [
    "new_context = ['Machine Learning is the study of algorithms which learn with more data',\n",
    "               'AI is known as Artificial Intelligence'] + retrieved_context\n",
    "new_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13ea9336-a615-45cb-924c-64c9342f82d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9005,
     "status": "ok",
     "timestamp": 1734095853038,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "2Urkt1YHIuK8",
    "outputId": "8736b357-0964-4c4c-b00c-d008ea353895"
   },
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=response['response'],\n",
    "    expected_output=human_answer,\n",
    "    retrieval_context=new_context\n",
    ")\n",
    "\n",
    "metric = ContextualPrecisionMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95df2e5e-7300-4868-ae1c-dd3b9feb7a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1734095863827,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "00qrXrl9JY06",
    "outputId": "07ae6595-2dcc-4908-9d5b-7fd5da06e859"
   },
   "outputs": [],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d171d551-224f-46b0-89da-338999a49bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "uTDmP7-dLRlp"
   },
   "source": [
    "## Contextual Recall\n",
    "\n",
    "The contextual recall metric measures the quality of your RAG pipeline's retriever by evaluating the extent of which the `retrieval_context` aligns with the `expected_output`.\n",
    "\n",
    "`deepeval`'s contextual recall metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score using an LLM as a Judge.\n",
    "\n",
    "In `deepeval`, to use the ContextualRecallMetric, you'll have to provide the following arguments when creating an `LLMTestCase`:\n",
    "\n",
    "- `input` : Input Query (not used in the computation)\n",
    "- `actual_output` : Actual LLM Response (not used in the computation)\n",
    "- `expected_output` : Expected LLM Response (ground truth answer)\n",
    "- `retrieval_context` : Top-N retrieved document chunks (nodes) from Vector DB\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/PDbwuX5.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3856d0f0-5970-422f-878f-75f0ba81196b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VhnDaJvELRlq",
    "outputId": "c2cc9da2-ca24-41f6-a7d0-53890dacb488"
   },
   "outputs": [],
   "source": [
    "query = \"What is AI?\"\n",
    "response = rag_chain_w_sources.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1706141-cec1-4f6c-a733-598dc9e2f217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "SRdTNvgESiaC"
   },
   "source": [
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ce87eea-5b8c-488a-b35d-ffa7184a6049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uv7FKS6XLRlr",
    "outputId": "76f5611a-9ed0-4120-dd20-abbfafef7221"
   },
   "outputs": [],
   "source": [
    "retrieved_context = [doc.page_content for doc in response['context']]\n",
    "retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "012c0c29-3be5-4006-9b77-a52f98250666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "90_guzmVLRls"
   },
   "outputs": [],
   "source": [
    "human_answer = \"\"\"AI, also known as Artificial Intelligence is used to build complex systems for applications\n",
    "                  like virtual assistants, robotics and autonomous vehicles.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "505c658c-4fb3-4a33-a19d-8c380403934f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84eNVGrwLRlt",
    "outputId": "f522e7cd-e815-4634-a180-9e3347630fe1"
   },
   "outputs": [],
   "source": [
    "new_context = ['NVIDIA makes chips for AI', 'AI is an acronym for Artificial Intellence']\n",
    "new_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9fba24a-0264-4a5f-aa33-244b13b511ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Of-wta5OLRlt",
    "outputId": "267e8858-71f1-41a8-d27d-e80f5f635cd8"
   },
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "test_case1 = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=response['response'],\n",
    "    expected_output=human_answer,\n",
    "    retrieval_context=retrieved_context\n",
    ")\n",
    "\n",
    "test_case2 = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=response['response'],\n",
    "    expected_output=human_answer,\n",
    "    retrieval_context=new_context\n",
    ")\n",
    "\n",
    "metric = ContextualRecallMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "result = evaluate([test_case1, test_case2], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc205eaf-804d-4bbb-903a-908434645930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13c3vhCQLRlt",
    "outputId": "bcae6817-0a1c-4ec6-ae51-7b2ca301be03"
   },
   "outputs": [],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd924b8c-720f-44d8-9ee6-552d4e29077d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-bbC5jelN9v",
    "outputId": "f86d6460-af92-4cb3-c90f-d6b8984cec82"
   },
   "outputs": [],
   "source": [
    "print('Sucess:', result.test_results[1].metrics_data[0].success)\n",
    "print('Score:', result.test_results[1].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[1].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47140171-afc1-4cdf-9102-f1a3a91df344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "SfHpMTVvSx7h"
   },
   "source": [
    "## Contextual Relevancy\n",
    "\n",
    "The contextual relevancy metric measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your `retrieval_context` for a given `input`.\n",
    "\n",
    "`deepeval`'s contextual relevancy metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score using an LLM as a Judge.\n",
    "\n",
    "In `deepeval`, to use the ContextualRelevancyMetric, you'll have to provide the following arguments when creating an `LLMTestCase`:\n",
    "\n",
    "- `input` : Input Query\n",
    "- `actual_output` : Actual LLM Response (not used in the computation)\n",
    "- `retrieval_context` : Top-N retrieved document chunks (nodes) from Vector DB\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/VLKoEsI.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81abe1cd-1d29-4cb0-bf08-b947d965a0a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QI5H2AWqSx7h",
    "outputId": "89ad1a10-6299-4a0b-cea1-49bdbea2e0f5"
   },
   "outputs": [],
   "source": [
    "query = \"What is AI?\"\n",
    "response = rag_chain_w_sources.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55aaa3ff-5c22-4ef0-964c-1f3b33db39e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "45UdHqalSx7i"
   },
   "source": [
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3d78c36-f7ab-48b5-a080-d812641b4454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOpExtxiSx7i",
    "outputId": "2420dba1-4969-45df-923a-5ce5577f60af"
   },
   "outputs": [],
   "source": [
    "retrieved_context = [doc.page_content for doc in response['context']]\n",
    "retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7de2e77e-508e-4c3a-bbed-e3c7ef62dddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xyAtudlSSx7k",
    "outputId": "9d4a3240-3268-4f5e-d61b-3f8e55c0d51a"
   },
   "outputs": [],
   "source": [
    "new_context = ['NVIDIA makes chips for AI', 'Google and Microsoft are battling out the market share for AI Chatbots'] + retrieved_context\n",
    "new_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70df5754-f978-4aa1-af42-957c70dca14d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2_fQqaFUSx7l",
    "outputId": "34468573-ef29-489e-8b4d-1cc27cca3714"
   },
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=response['response'],\n",
    "    expected_output=human_answer,\n",
    "    retrieval_context=new_context\n",
    ")\n",
    "\n",
    "metric = ContextualRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31ff3d07-9152-45ee-a728-d32c2ae8bc38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NnZIKnxmSx7l",
    "outputId": "f73af640-72dd-4949-d462-c9219cd35168"
   },
   "outputs": [],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "M7_Deep_Dive_into_RAG_Evaluation_Metrics",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a1982c87e18425a97c43a61c18f928b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28c6583c344341599988c3dad3070cc8",
      "placeholder": "​",
      "style": "IPY_MODEL_9d4be022d51847e8a1ccd0a9eb885de2",
      "value": "Evaluating: 100%"
     }
    },
    "28c6583c344341599988c3dad3070cc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7614ca4667f948ccba8f980a1ae4722d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8890484b6c3e4018907b96b026510712": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0a1982c87e18425a97c43a61c18f928b",
       "IPY_MODEL_d7e48662099b409baeb15177af124bb5",
       "IPY_MODEL_d3d8515a3b194fb3b1fdc820607a00bf"
      ],
      "layout": "IPY_MODEL_ce0f9b8f431246da93ebbe3c5cb8b026"
     }
    },
    "9d4be022d51847e8a1ccd0a9eb885de2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab7499442d7648168ca9415bda596e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ad467cc9f9884950ada71fbb95cfccbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5bd0a78f28749e39a63421f0da20446": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce0f9b8f431246da93ebbe3c5cb8b026": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3d8515a3b194fb3b1fdc820607a00bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad467cc9f9884950ada71fbb95cfccbd",
      "placeholder": "​",
      "style": "IPY_MODEL_c5bd0a78f28749e39a63421f0da20446",
      "value": " 1/1 [00:03&lt;00:00,  3.12s/it]"
     }
    },
    "d7e48662099b409baeb15177af124bb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7614ca4667f948ccba8f980a1ae4722d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab7499442d7648168ca9415bda596e05",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
