{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e54c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20310c3c",
   "metadata": {},
   "source": [
    "## Example 1: Basic Contextual Precision Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a084a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5bea77d8d3428e9ebf3d595900bab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Precision Score: 0.756\n",
      "\n",
      "Reason: The score is 0.76 because the relevant nodes are generally ranked higher than the irrelevant ones. The first node correctly highlights Python's 'simple and readable syntax,' which is a key feature. However, the second node, ranked second, discusses Java, which is unrelated to Python's features. This lowers the score as it should be ranked lower. The third node effectively mentions Python's support for 'multiple programming paradigms,' aligning well with the input. The fourth node, discussing JavaScript, is again irrelevant and should be ranked lower. Finally, the fifth node correctly emphasizes Python's 'vast ecosystem of libraries,' supporting the input. The presence of irrelevant nodes in higher ranks prevents a higher score.\n",
      "\n",
      "Success (>= threshold): True\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Define the test case\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What are the key features of Python programming language?\",\n",
    "    \n",
    "    # The actual output from your RAG system\n",
    "    actual_output=\"\"\"Python is a high-level, interpreted programming language \n",
    "    with simple syntax. It supports multiple programming paradigms and has \n",
    "    extensive libraries.\"\"\",\n",
    "    \n",
    "    # Ground truth - what the complete answer should be\n",
    "    expected_output=\"\"\"Python is a high-level, interpreted language with simple \n",
    "    and readable syntax, supports multiple programming paradigms including \n",
    "    procedural, object-oriented, and functional programming, and has extensive \n",
    "    libraries for various applications.\"\"\",\n",
    "    \n",
    "    # Retrieved context from your retrieval system (ORDER MATTERS!)\n",
    "    retrieval_context=[\n",
    "        \"Python is a high-level, interpreted programming language known for its simple and readable syntax. It uses indentation for code blocks.\",\n",
    "        \"Java was developed by Sun Microsystems in 1995 and is widely used for enterprise applications.\",\n",
    "        \"Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.\",\n",
    "        \"JavaScript is primarily used for web development and runs in web browsers.\",\n",
    "        \"Python has a vast ecosystem of libraries like Django, NumPy, and Pandas for various applications.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the metric\n",
    "contextual_precision_metric = ContextualPrecisionMetric(\n",
    "    threshold=0.7,  # Minimum acceptable score\n",
    "    model=\"gpt-4o\",  # LLM model to use for evaluation\n",
    "    include_reason=True  # Include reasoning in the output\n",
    ")\n",
    "\n",
    "# Measure the metric\n",
    "contextual_precision_metric.measure(test_case)\n",
    "\n",
    "# Print results\n",
    "print(f\"Contextual Precision Score: {contextual_precision_metric.score:.3f}\")\n",
    "print(f\"\\nReason: {contextual_precision_metric.reason}\")\n",
    "print(f\"\\nSuccess (>= threshold): {contextual_precision_metric.is_successful()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583fb4b1",
   "metadata": {},
   "source": [
    "## Example 2: Perfect Contextual Precision (High Score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0879dda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9e5224ab6c49d6bd7f28f81d4a7549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Precision Score: 0.950\n",
      "\n",
      "Reason: The score is 0.95 because the first three nodes in the retrieval contexts are highly relevant, providing key features of Python such as 'simple and readable syntax,' 'support for multiple programming paradigms,' and 'a vast ecosystem of libraries.' These nodes are appropriately ranked higher. However, the fourth node, ranked fourth, is less relevant as it focuses on 'Python's usage in various fields' rather than its intrinsic features, which slightly affects the score. The fifth node returns to relevance by highlighting 'readability and extensive standard library,' maintaining a high overall precision.\n",
      "\n",
      "Success (>= threshold): True\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Test case with perfect retrieval and ranking\n",
    "test_case_perfect = LLMTestCase(\n",
    "    input=\"What are the key features of Python programming language?\",\n",
    "    \n",
    "    actual_output=\"\"\"Python is a high-level, interpreted language with readable \n",
    "    syntax, supports multiple paradigms, and has extensive libraries.\"\"\",\n",
    "    \n",
    "    expected_output=\"\"\"Python is a high-level, interpreted language with simple \n",
    "    syntax, supports multiple programming paradigms, and has extensive libraries.\"\"\",\n",
    "    \n",
    "    # All chunks are relevant AND well-ordered (most relevant first)\n",
    "    retrieval_context=[\n",
    "        \"Python is a high-level, interpreted programming language known for its simple and readable syntax.\",\n",
    "        \"Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.\",\n",
    "        \"Python has a vast ecosystem of libraries like Django, Flask, NumPy, and Pandas.\",\n",
    "        \"Python is widely used in web development, data science, machine learning, and automation.\",\n",
    "        \"Python's readability and extensive standard library make it beginner-friendly.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize and measure\n",
    "contextual_precision_metric = ContextualPrecisionMetric(\n",
    "    threshold=0.9,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextual_precision_metric.measure(test_case_perfect)\n",
    "\n",
    "# Print results\n",
    "print(f\"Contextual Precision Score: {contextual_precision_metric.score:.3f}\")\n",
    "print(f\"\\nReason: {contextual_precision_metric.reason}\")\n",
    "print(f\"\\nSuccess (>= threshold): {contextual_precision_metric.is_successful()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0542d3b8",
   "metadata": {},
   "source": [
    "## Example 3: Poor Contextual Precision (Low Score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be732818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33e224381754b81aadc03c8b8f5d95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Precision Score: 0.250\n",
      "\n",
      "Reason: The score is 0.25 because the relevant node, ranked fourth, clearly states \"Paris is the capital and most populous city of France,\" directly answering the input question. However, it is ranked lower than three irrelevant nodes. The first node, \"France is a country in Western Europe known for its wine and cheese,\" does not address the capital city. The second node, \"The French Revolution began in 1789 and lasted until 1799,\" focuses on a historical event unrelated to the capital. The third node, \"French is a Romance language derived from Latin,\" discusses the language rather than the capital city. These irrelevant nodes should be ranked lower to improve the score.\n",
      "\n",
      "Success (>= threshold): False\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Test case with poor retrieval - many irrelevant chunks\n",
    "test_case_poor = LLMTestCase(\n",
    "    input=\"What is the capital of France?\",\n",
    "    \n",
    "    actual_output=\"The capital of France is Paris.\",\n",
    "    \n",
    "    expected_output=\"The capital of France is Paris.\",\n",
    "    \n",
    "    # Poor retrieval: mostly irrelevant chunks, relevant chunk buried\n",
    "    retrieval_context=[\n",
    "        \"France is a country in Western Europe known for its wine and cheese.\",\n",
    "        \"The French Revolution began in 1789 and lasted until 1799.\",\n",
    "        \"French is a Romance language derived from Latin.\",\n",
    "        \"Paris is the capital and most populous city of France.\",  # Only relevant chunk\n",
    "        \"The Eiffel Tower was built in 1889 for the World's Fair.\",\n",
    "        \"France is a founding member of the European Union.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Measure\n",
    "metric = ContextualPrecisionMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "metric.measure(test_case_poor)\n",
    "\n",
    "print(f\"Contextual Precision Score: {metric.score:.3f}\")\n",
    "print(f\"\\nReason: {metric.reason}\")\n",
    "print(f\"\\nSuccess (>= threshold): {metric.is_successful()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45b9d9",
   "metadata": {},
   "source": [
    "![Contextual Precision Diagram](../image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54dcc56",
   "metadata": {},
   "source": [
    "## Example 4: Medical Query with Detailed Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780284fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca65673fd02c4165927f88678f8fc8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MEDICAL QUERY EVALUATION\n",
      "======================================================================\n",
      "Contextual Precision Score: 0.756\n",
      "\n",
      "Reason:\n",
      "The score is 0.76 because the relevant nodes, such as the first node mentioning \"increased thirst (polydipsia) and frequent urination (polyuria)\" and the third node stating \"fatigue and unexplained weight loss,\" are ranked higher than some irrelevant nodes. However, the second node, which is about Type 1 Diabetes, and the fourth node discussing gestational diabetes, are ranked higher than the fifth node, which correctly mentions \"blurred vision and slow-healing wounds.\" This misplacement of irrelevant nodes above relevant ones affects the score.\n",
      "\n",
      "Passed Threshold (0.6): True\n",
      "\n",
      "======================================================================\n",
      "MANUAL RELEVANCE ANALYSIS\n",
      "======================================================================\n",
      "Rank 1: Type 2 symptoms (thirst, urination) - RELEVANT ‚úì\n",
      "Rank 2: Type 1 Diabetes info - IRRELEVANT ‚úó\n",
      "Rank 3: Type 2 symptoms (fatigue, weight loss) - RELEVANT ‚úì\n",
      "Rank 4: Gestational diabetes info - IRRELEVANT ‚úó\n",
      "Rank 5: Type 2 symptoms (blurred vision, wounds) - RELEVANT ‚úì\n",
      "Rank 6: Diabetes management - PARTIALLY RELEVANT ~\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Medical information retrieval scenario\n",
    "test_case_medical = LLMTestCase(\n",
    "    input=\"What are the symptoms of Type 2 Diabetes?\",\n",
    "    \n",
    "    actual_output=\"\"\"Type 2 Diabetes symptoms include increased thirst, frequent \n",
    "    urination, fatigue, blurred vision, and slow-healing wounds.\"\"\",\n",
    "    \n",
    "    expected_output=\"\"\"Type 2 Diabetes presents with symptoms including increased \n",
    "    thirst, frequent urination, unexplained weight loss, fatigue, blurred vision, \n",
    "    and slow-healing wounds.\"\"\",\n",
    "    \n",
    "    retrieval_context=[\n",
    "        \"Type 2 Diabetes symptoms include increased thirst (polydipsia) and frequent urination (polyuria).\",\n",
    "        \"Type 1 Diabetes is an autoimmune condition where the pancreas produces little or no insulin.\",\n",
    "        \"Patients with Type 2 Diabetes often experience fatigue and unexplained weight loss.\",\n",
    "        \"Gestational diabetes develops during pregnancy and usually resolves after delivery.\",\n",
    "        \"Blurred vision and slow-healing wounds are common symptoms of Type 2 Diabetes.\",\n",
    "        \"Regular exercise and a balanced diet can help manage diabetes.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Measure contextual precision\n",
    "metric = ContextualPrecisionMetric(\n",
    "    threshold=0.6,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "metric.measure(test_case_medical)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MEDICAL QUERY EVALUATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Contextual Precision Score: {metric.score:.3f}\")\n",
    "print(f\"\\nReason:\\n{metric.reason}\")\n",
    "print(f\"\\nPassed Threshold (0.6): {metric.is_successful()}\")\n",
    "\n",
    "# Manual breakdown\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MANUAL RELEVANCE ANALYSIS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"Rank 1: Type 2 symptoms (thirst, urination) - RELEVANT ‚úì\")\n",
    "print(\"Rank 2: Type 1 Diabetes info - IRRELEVANT ‚úó\")\n",
    "print(\"Rank 3: Type 2 symptoms (fatigue, weight loss) - RELEVANT ‚úì\")\n",
    "print(\"Rank 4: Gestational diabetes info - IRRELEVANT ‚úó\")\n",
    "print(\"Rank 5: Type 2 symptoms (blurred vision, wounds) - RELEVANT ‚úì\")\n",
    "print(\"Rank 6: Diabetes management - PARTIALLY RELEVANT ~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ca318",
   "metadata": {},
   "source": [
    "![Contextual Precision Diagram](../image_copy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e04ff0",
   "metadata": {},
   "source": [
    "## Example 5: Batch Evaluation with Multiple Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cae4b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153d55ff20154b33aef7f42e1c8f1238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BATCH EVALUATION RESULTS\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afedd99c697a4402a137c20763b43d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case 1:\n",
      "  Query: What is the capital of Japan?\n",
      "  Contextual Precision: 1.000\n",
      "  Passed (>=0.7): True\n",
      "  Reason: The score is 1.00 because the relevant node, ranked first, directly answers the question with \"Tokyo...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378c446a59d94ed28c3f026905bb1564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case 2:\n",
      "  Query: Who invented the telephone?\n",
      "  Contextual Precision: 0.500\n",
      "  Passed (>=0.7): False\n",
      "  Reason: The score is 0.50 because the relevant node, ranked second, \"Alexander Graham Bell is credited with ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case 3:\n",
      "  Query: What are the primary colors?\n",
      "  Contextual Precision: 1.000\n",
      "  Passed (>=0.7): True\n",
      "  Reason: The score is 1.00 because the first node in the retrieval contexts directly answers the input questi...\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Create multiple test cases with varying precision quality\n",
    "test_cases = [\n",
    "    # Case 1: Good precision\n",
    "    LLMTestCase(\n",
    "        input=\"What is the capital of Japan?\",\n",
    "        expected_output=\"The capital of Japan is Tokyo.\",\n",
    "        retrieval_context=[\n",
    "            \"Tokyo is the capital and largest city of Japan.\",\n",
    "            \"Japan is an island nation in East Asia.\",\n",
    "            \"Tokyo was formerly known as Edo before 1868.\"\n",
    "        ]\n",
    "    ),\n",
    "    \n",
    "    # Case 2: Poor precision (irrelevant chunks)\n",
    "    LLMTestCase(\n",
    "        input=\"Who invented the telephone?\",\n",
    "        expected_output=\"Alexander Graham Bell invented the telephone in 1876.\",\n",
    "        retrieval_context=[\n",
    "            \"Thomas Edison invented the light bulb in 1879.\",\n",
    "            \"Alexander Graham Bell is credited with inventing the telephone in 1876.\",\n",
    "            \"The Wright brothers invented the airplane in 1903.\"\n",
    "        ]\n",
    "    ),\n",
    "    \n",
    "    # Case 3: Perfect precision\n",
    "    LLMTestCase(\n",
    "        input=\"What are the primary colors?\",\n",
    "        expected_output=\"The primary colors are red, blue, and yellow.\",\n",
    "        retrieval_context=[\n",
    "            \"The primary colors are red, blue, and yellow.\",\n",
    "            \"These three colors cannot be created by mixing other colors.\",\n",
    "            \"All other colors can be created by mixing primary colors.\"\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define metric\n",
    "contextual_precision = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "# Evaluate each test case\n",
    "print(\"=\"*70)\n",
    "print(\"BATCH EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    # Measure metric\n",
    "    contextual_precision.measure(test_case)\n",
    "    \n",
    "    print(f\"\\nTest Case {i}:\")\n",
    "    print(f\"  Query: {test_case.input}\")\n",
    "    print(f\"  Contextual Precision: {contextual_precision.score:.3f}\")\n",
    "    print(f\"  Passed (>=0.7): {contextual_precision.is_successful()}\")\n",
    "    print(f\"  Reason: {contextual_precision.reason[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08723e42",
   "metadata": {},
   "source": [
    "## Example 6: Comparing Good vs Bad Ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d645892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b3ccf9bb2546f78966db679e455b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81918c84d9024d85bdaac5ceaa1f35af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RANKING QUALITY COMPARISON\n",
      "======================================================================\n",
      "\n",
      "üìä SCENARIO A: Good Ranking (Relevant chunks first)\n",
      "   Score: 1.000\n",
      "   Reason: The score is 1.00 because all relevant nodes are correctly ranked higher than irrelevant ones. The first three nodes provide precise information about Apache Spark's use in 'large-scale data processing,' 'real-time stream processing,' and 'machine learning,' which are directly aligned with the input query. The irrelevant nodes, such as the fourth node discussing 'Apache Hadoop' and the fifth node about 'Kafka,' are appropriately ranked lower, as they do not pertain to Apache Spark. Great job on maintaining perfect precision!\n",
      "\n",
      "üìä SCENARIO B: Bad Ranking (Irrelevant chunks first)\n",
      "   Score: 0.478\n",
      "   Reason: The score is 0.48 because the first two nodes in the retrieval contexts are irrelevant to the input. The first node discusses Apache Hadoop and MapReduce, which are not directly related to Apache Spark's uses, and the second node describes Kafka, a different technology. These irrelevant nodes are ranked higher than the relevant nodes, which include the third node stating 'Apache Spark is a unified analytics engine for large-scale data processing,' the fourth node mentioning 'Spark is widely used for real-time stream processing and batch processing,' and the fifth node highlighting 'Spark MLlib provides machine learning algorithms for distributed computing.' These relevant nodes should be ranked higher to improve the score.\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS\n",
      "======================================================================\n",
      "Score Difference: 0.522\n",
      "Impact of Bad Ranking: 52.2% degradation\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Same query, same relevant chunks, but different ranking\n",
    "\n",
    "# Scenario A: Good Ranking (relevant chunks first)\n",
    "test_case_good_ranking = LLMTestCase(\n",
    "    input=\"What is Apache Spark used for?\",\n",
    "    expected_output=\"Apache Spark is used for big data processing, real-time analytics, and machine learning.\",\n",
    "    retrieval_context=[\n",
    "        \"Apache Spark is a unified analytics engine for large-scale data processing.\",  # Relevant ‚úì\n",
    "        \"Spark is widely used for real-time stream processing and batch processing.\",  # Relevant ‚úì\n",
    "        \"Spark MLlib provides machine learning algorithms for distributed computing.\",  # Relevant ‚úì\n",
    "        \"Apache Hadoop is another big data framework that uses MapReduce.\",  # Less relevant\n",
    "        \"Kafka is a distributed streaming platform for building real-time data pipelines.\"  # Irrelevant\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scenario B: Bad Ranking (irrelevant chunks first)\n",
    "test_case_bad_ranking = LLMTestCase(\n",
    "    input=\"What is Apache Spark used for?\",\n",
    "    expected_output=\"Apache Spark is used for big data processing, real-time analytics, and machine learning.\",\n",
    "    retrieval_context=[\n",
    "        \"Apache Hadoop is another big data framework that uses MapReduce.\",  # Less relevant\n",
    "        \"Kafka is a distributed streaming platform for building real-time data pipelines.\",  # Irrelevant\n",
    "        \"Apache Spark is a unified analytics engine for large-scale data processing.\",  # Relevant ‚úì\n",
    "        \"Spark is widely used for real-time stream processing and batch processing.\",  # Relevant ‚úì\n",
    "        \"Spark MLlib provides machine learning algorithms for distributed computing.\"  # Relevant ‚úì\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate both\n",
    "metric_good = ContextualPrecisionMetric(threshold=0.7, model=\"gpt-4o\", include_reason=True)\n",
    "metric_bad = ContextualPrecisionMetric(threshold=0.7, model=\"gpt-4o\", include_reason=True)\n",
    "\n",
    "metric_good.measure(test_case_good_ranking)\n",
    "metric_bad.measure(test_case_bad_ranking)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RANKING QUALITY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä SCENARIO A: Good Ranking (Relevant chunks first)\")\n",
    "print(f\"   Score: {metric_good.score:.3f}\")\n",
    "print(f\"   Reason: {metric_good.reason}\")\n",
    "\n",
    "print(\"\\nüìä SCENARIO B: Bad Ranking (Irrelevant chunks first)\")\n",
    "print(f\"   Score: {metric_bad.score:.3f}\")\n",
    "print(f\"   Reason: {metric_bad.reason}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Score Difference: {abs(metric_good.score - metric_bad.score):.3f}\")\n",
    "print(f\"Impact of Bad Ranking: {((metric_good.score - metric_bad.score) / metric_good.score * 100):.1f}% degradation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6441015",
   "metadata": {},
   "source": [
    "![Contextual Precision Diagram](../image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22fb8c6",
   "metadata": {},
   "source": [
    "## Example 7: Real-World RAG Pipeline Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b80f0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a73ea5e79c4dd7920243f482c1ae8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RAG PIPELINE EVALUATION - CONTEXTUAL PRECISION\n",
      "======================================================================\n",
      "\n",
      "üìù Query: What is Databricks and what are its key features?\n",
      "\n",
      "üéØ Ground Truth:\n",
      "Databricks is a unified analytics platform built on Apache Spark. \n",
      "Its key features include collaborative notebooks, Lakehouse architecture, Delta Lake \n",
      "support, machine learning capabilities, and automated cluster management.\n",
      "\n",
      "ü§ñ Generated Answer:\n",
      "Based on the retrieved information: Databricks is a unified analytics platform built on Apache Spark....\n",
      "\n",
      "üìö Retrieved Context (in ranking order):\n",
      "   [1] Databricks is a unified analytics platform built on Apache Spark.\n",
      "   [2] Databricks Lakehouse combines data warehouse and data lake capabilities.\n",
      "   [3] Databricks offers automated cluster management and scaling.\n",
      "   [4] Databricks provides collaborative notebooks for data science teams.\n",
      "   [5] Apache Spark is an open-source distributed computing system.\n",
      "   [6] Python is a popular programming language for data analysis.\n",
      "\n",
      "üìä Contextual Precision Score: 1.000\n",
      "\n",
      "üí° Evaluation Reason:\n",
      "The score is 1.00 because all relevant nodes are perfectly ranked higher than irrelevant ones. The first four nodes provide precise information about Databricks, such as 'Databricks is a unified analytics platform built on Apache Spark' (first node) and 'Databricks offers automated cluster management and scaling' (third node). The irrelevant nodes, like the fifth node stating 'Apache Spark is an open-source distributed computing system,' are correctly placed lower, ensuring a flawless ranking.\n",
      "\n",
      "‚úÖ Passed Threshold (0.75): True\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDATIONS FOR IMPROVEMENT\n",
      "======================================================================\n",
      "‚úÖ Contextual Precision is good (>= 0.8)\n",
      "   - Continue monitoring with production queries\n",
      "   - Consider A/B testing with different retrieval parameters\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from typing import List, Dict\n",
    "\n",
    "# Simulate a RAG pipeline with vector search\n",
    "class SimpleRAGPipeline:\n",
    "    def __init__(self, knowledge_base: List[str]):\n",
    "        self.knowledge_base = knowledge_base\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Simulated retrieval with relevance scoring\n",
    "        In real scenario, this would be vector similarity search\n",
    "        \"\"\"\n",
    "        # Simple keyword-based relevance (for demonstration)\n",
    "        query_terms = set(query.lower().split())\n",
    "        \n",
    "        scored_docs = []\n",
    "        for doc in self.knowledge_base:\n",
    "            doc_terms = set(doc.lower().split())\n",
    "            # Simple overlap score\n",
    "            score = len(query_terms & doc_terms) / len(query_terms)\n",
    "            scored_docs.append((doc, score))\n",
    "        \n",
    "        # Sort by score (descending) and return top_k\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, score in scored_docs[:top_k]]\n",
    "    \n",
    "    def generate(self, query: str, context: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Simulated generation (in real scenario, this would use LLM)\n",
    "        \"\"\"\n",
    "        return f\"Based on the retrieved information: {context[0][:100]}...\"\n",
    "\n",
    "# Create knowledge base\n",
    "knowledge_base = [\n",
    "    \"Databricks is a unified analytics platform built on Apache Spark.\",\n",
    "    \"Databricks provides collaborative notebooks for data science teams.\",\n",
    "    \"Apache Spark is an open-source distributed computing system.\",\n",
    "    \"Python is a popular programming language for data analysis.\",\n",
    "    \"Databricks Lakehouse combines data warehouse and data lake capabilities.\",\n",
    "    \"SQL is used for querying structured data in databases.\",\n",
    "    \"Databricks supports Delta Lake for reliable data lakes.\",\n",
    "    \"Machine learning models can be trained on Databricks.\",\n",
    "    \"Java is an object-oriented programming language.\",\n",
    "    \"Databricks offers automated cluster management and scaling.\"\n",
    "]\n",
    "\n",
    "# Initialize RAG pipeline\n",
    "rag_pipeline = SimpleRAGPipeline(knowledge_base)\n",
    "\n",
    "# Test query\n",
    "query = \"What is Databricks and what are its key features?\"\n",
    "ground_truth = \"\"\"Databricks is a unified analytics platform built on Apache Spark. \n",
    "Its key features include collaborative notebooks, Lakehouse architecture, Delta Lake \n",
    "support, machine learning capabilities, and automated cluster management.\"\"\"\n",
    "\n",
    "# Retrieve context\n",
    "retrieved_context = rag_pipeline.retrieve(query, top_k=6)\n",
    "\n",
    "# Generate answer\n",
    "generated_answer = rag_pipeline.generate(query, retrieved_context)\n",
    "\n",
    "# Create test case\n",
    "test_case = LLMTestCase(\n",
    "    input=query,\n",
    "    actual_output=generated_answer,\n",
    "    expected_output=ground_truth,\n",
    "    retrieval_context=retrieved_context\n",
    ")\n",
    "\n",
    "# Evaluate contextual precision\n",
    "metric = ContextualPrecisionMetric(\n",
    "    threshold=0.75,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*70)\n",
    "print(\"RAG PIPELINE EVALUATION - CONTEXTUAL PRECISION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìù Query: {query}\")\n",
    "print(f\"\\nüéØ Ground Truth:\\n{ground_truth}\")\n",
    "print(f\"\\nü§ñ Generated Answer:\\n{generated_answer}\")\n",
    "\n",
    "print(f\"\\nüìö Retrieved Context (in ranking order):\")\n",
    "for i, doc in enumerate(retrieved_context, 1):\n",
    "    print(f\"   [{i}] {doc}\")\n",
    "\n",
    "print(f\"\\nüìä Contextual Precision Score: {metric.score:.3f}\")\n",
    "print(f\"\\nüí° Evaluation Reason:\\n{metric.reason}\")\n",
    "print(f\"\\n‚úÖ Passed Threshold (0.75): {metric.is_successful()}\")\n",
    "\n",
    "# Provide recommendations\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RECOMMENDATIONS FOR IMPROVEMENT\")\n",
    "print(f\"{'='*70}\")\n",
    "if metric.score < 0.8:\n",
    "    print(\"‚ö†Ô∏è  Contextual Precision is below optimal (< 0.8)\")\n",
    "    print(\"   - Review retrieval algorithm to filter irrelevant chunks\")\n",
    "    print(\"   - Improve ranking by tuning similarity thresholds\")\n",
    "    print(\"   - Consider using re-ranking models\")\n",
    "    print(\"   - Reduce top_k parameter to retrieve fewer but more relevant chunks\")\n",
    "else:\n",
    "    print(\"‚úÖ Contextual Precision is good (>= 0.8)\")\n",
    "    print(\"   - Continue monitoring with production queries\")\n",
    "    print(\"   - Consider A/B testing with different retrieval parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103be9e5",
   "metadata": {},
   "source": [
    "## Example 8: Understanding LLM-Based Relevance Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "198650ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee63652384734c50923fe7634267771c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LLM-BASED RELEVANCE ASSESSMENT DEMONSTRATION\n",
      "======================================================================\n",
      "\n",
      "üîç How DeepEval Evaluates Each Node:\n",
      "\n",
      "The LLM asks for each node: 'Is this node relevant for answering:\n",
      "  'How does climate change affect ocean levels?' given the expected answer?'\n",
      "\n",
      "Node 1: 'Rising global temperatures cause ocean water to expand...'\n",
      "  LLM Assessment: RELEVANT ‚úì\n",
      "  Reasoning: Directly explains thermal expansion mechanism\n",
      "\n",
      "Node 2: 'Photosynthesis is the process...'\n",
      "  LLM Assessment: IRRELEVANT ‚úó\n",
      "  Reasoning: About plant biology, not ocean levels\n",
      "\n",
      "Node 3: 'Melting glaciers and ice sheets...'\n",
      "  LLM Assessment: RELEVANT ‚úì\n",
      "  Reasoning: Directly explains ice melting mechanism\n",
      "\n",
      "Node 4: 'The Amazon rainforest...'\n",
      "  LLM Assessment: IRRELEVANT ‚úó\n",
      "  Reasoning: About rainforest, not ocean levels\n",
      "\n",
      "Node 5: 'Warmer ocean temperatures also lead to hurricanes...'\n",
      "  LLM Assessment: PARTIALLY RELEVANT ~\n",
      "  Reasoning: Related to climate change effects but not about sea level rise\n",
      "\n",
      "======================================================================\n",
      "üìä Final Contextual Precision Score: 0.833\n",
      "======================================================================\n",
      "\n",
      "The score is 0.83 because the relevant nodes are generally ranked higher than the irrelevant ones. The first node effectively highlights 'Rising global temperatures cause ocean water to expand, a phenomenon known as thermal expansion,' which is directly pertinent to the input. However, the second node, ranked second, discusses 'Photosynthesis is the process by which plants convert sunlight into energy,' which is unrelated to the topic of ocean levels and climate change. This irrelevant node should be ranked lower. The third node, 'Melting glaciers and ice sheets on land contribute water to the oceans, raising sea levels,' is correctly ranked as it directly supports the input. The fourth node, 'The Amazon rainforest is often called the lungs of the Earth,' is unrelated and should be ranked lower. Lastly, the fifth node, 'Warmer ocean temperatures also lead to more frequent and intense hurricanes,' does not directly address the mechanisms causing ocean levels to rise, indicating it should be ranked lower as well. Overall, the score reflects a good but not perfect ranking of relevant information.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Demonstrate how DeepEval uses LLM to assess relevance\n",
    "test_case = LLMTestCase(\n",
    "    input=\"How does climate change affect ocean levels?\",\n",
    "    \n",
    "    expected_output=\"\"\"Climate change causes ocean levels to rise through two main \n",
    "    mechanisms: thermal expansion of water as oceans warm, and melting of land-based \n",
    "    ice such as glaciers and ice sheets.\"\"\",\n",
    "    \n",
    "    retrieval_context=[\n",
    "        \"Rising global temperatures cause ocean water to expand, a phenomenon known as thermal expansion.\",\n",
    "        \"Photosynthesis is the process by which plants convert sunlight into energy.\",\n",
    "        \"Melting glaciers and ice sheets on land contribute water to the oceans, raising sea levels.\",\n",
    "        \"The Amazon rainforest is often called the lungs of the Earth.\",\n",
    "        \"Warmer ocean temperatures also lead to more frequent and intense hurricanes.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create metric\n",
    "metric = ContextualPrecisionMetric(\n",
    "    threshold=0.6,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LLM-BASED RELEVANCE ASSESSMENT DEMONSTRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüîç How DeepEval Evaluates Each Node:\\n\")\n",
    "print(\"The LLM asks for each node: 'Is this node relevant for answering:\")\n",
    "print(f\"  '{test_case.input}' given the expected answer?'\\n\")\n",
    "\n",
    "print(\"Node 1: 'Rising global temperatures cause ocean water to expand...'\")\n",
    "print(\"  LLM Assessment: RELEVANT ‚úì\")\n",
    "print(\"  Reasoning: Directly explains thermal expansion mechanism\\n\")\n",
    "\n",
    "print(\"Node 2: 'Photosynthesis is the process...'\")\n",
    "print(\"  LLM Assessment: IRRELEVANT ‚úó\")\n",
    "print(\"  Reasoning: About plant biology, not ocean levels\\n\")\n",
    "\n",
    "print(\"Node 3: 'Melting glaciers and ice sheets...'\")\n",
    "print(\"  LLM Assessment: RELEVANT ‚úì\")\n",
    "print(\"  Reasoning: Directly explains ice melting mechanism\\n\")\n",
    "\n",
    "print(\"Node 4: 'The Amazon rainforest...'\")\n",
    "print(\"  LLM Assessment: IRRELEVANT ‚úó\")\n",
    "print(\"  Reasoning: About rainforest, not ocean levels\\n\")\n",
    "\n",
    "print(\"Node 5: 'Warmer ocean temperatures also lead to hurricanes...'\")\n",
    "print(\"  LLM Assessment: PARTIALLY RELEVANT ~\")\n",
    "print(\"  Reasoning: Related to climate change effects but not about sea level rise\\n\")\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"üìä Final Contextual Precision Score: {metric.score:.3f}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{metric.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270cee0",
   "metadata": {},
   "source": [
    "## Final Code: Production-Ready Evaluation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "class RAGEvaluationPipeline:\n",
    "    \"\"\"Production-ready RAG evaluation for contextual precision\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.7, model: str = \"gpt-4\"):\n",
    "        self.metric = ContextualPrecisionMetric(\n",
    "            threshold=threshold,\n",
    "            model=model,\n",
    "            include_reason=True\n",
    "        )\n",
    "        self.results = []\n",
    "    \n",
    "    def evaluate_single(\n",
    "        self,\n",
    "        query: str,\n",
    "        expected_output: str,\n",
    "        retrieval_context: List[str],\n",
    "        actual_output: str = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"Evaluate a single query\"\"\"\n",
    "        \n",
    "        test_case = LLMTestCase(\n",
    "            input=query,\n",
    "            expected_output=expected_output,\n",
    "            retrieval_context=retrieval_context,\n",
    "            actual_output=actual_output or \"\"\n",
    "        )\n",
    "        \n",
    "        self.metric.measure(test_case)\n",
    "        \n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"score\": self.metric.score,\n",
    "            \"passed\": self.metric.is_successful(),\n",
    "            \"reason\": self.metric.reason,\n",
    "            \"num_chunks\": len(retrieval_context)\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def evaluate_batch(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"Evaluate multiple queries\"\"\"\n",
    "        \n",
    "        for case in test_cases:\n",
    "            self.evaluate_single(\n",
    "                query=case[\"query\"],\n",
    "                expected_output=case[\"expected_output\"],\n",
    "                retrieval_context=case[\"retrieval_context\"],\n",
    "                actual_output=case.get(\"actual_output\")\n",
    "            )\n",
    "        \n",
    "        return self.get_summary()\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get evaluation summary statistics\"\"\"\n",
    "        \n",
    "        if not self.results:\n",
    "            return {}\n",
    "        \n",
    "        scores = [r[\"score\"] for r in self.results]\n",
    "        passed = [r[\"passed\"] for r in self.results]\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": len(self.results),\n",
    "            \"avg_precision\": sum(scores) / len(scores),\n",
    "            \"min_precision\": min(scores),\n",
    "            \"max_precision\": max(scores),\n",
    "            \"pass_rate\": sum(passed) / len(passed) * 100,\n",
    "            \"failed_queries\": [\n",
    "                r[\"query\"] for r in self.results if not r[\"passed\"]\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def export_results(self, filename: str = \"precision_results.json\"):\n",
    "        \"\"\"Export results to JSON\"\"\"\n",
    "        \n",
    "        data = {\n",
    "            \"summary\": self.get_summary(),\n",
    "            \"detailed_results\": self.results\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        print(f\"Results exported to {filename}\")\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = RAGEvaluationPipeline(threshold=0.7, model=\"gpt-4\")\n",
    "    \n",
    "    # Test cases\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"query\": \"What is Databricks?\",\n",
    "            \"expected_output\": \"Databricks is a unified analytics platform built on Apache Spark.\",\n",
    "            \"retrieval_context\": [\n",
    "                \"Databricks is a unified analytics platform built on Apache Spark.\",\n",
    "                \"The platform provides collaborative notebooks and automated cluster management.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"What are Python's key features?\",\n",
    "            \"expected_output\": \"Python is a high-level language with simple syntax and extensive libraries.\",\n",
    "            \"retrieval_context\": [\n",
    "                \"Java is an object-oriented programming language.\",\n",
    "                \"Python is known for its simple and readable syntax.\",\n",
    "                \"Python has extensive libraries for various applications.\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Evaluate\n",
    "    summary = pipeline.evaluate_batch(test_cases)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total Queries: {summary['total_queries']}\")\n",
    "    print(f\"Average Precision: {summary['avg_precision']:.3f}\")\n",
    "    print(f\"Pass Rate: {summary['pass_rate']:.1f}%\")\n",
    "    print(f\"Failed Queries: {len(summary['failed_queries'])}\")\n",
    "    \n",
    "    # Export\n",
    "    pipeline.export_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36513bbd",
   "metadata": {},
   "source": [
    "## Code Example: Demonstrating Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d33a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6c492db9c44e4a84274279192316b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMONSTRATING CONTEXTUAL PRECISION DEPENDENCIES\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST 1: CHANGING RETRIEVED CONTEXT\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4df1a82aba344a18b7636eb89796e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c160476d33458885f3964de72faea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Context Score: 1.000\n",
      "Bad Context Score:  0.000\n",
      "Impact: 1.000 difference\n",
      "‚úÖ HUGE IMPACT - Retrieved Context is PRIMARY dependency\n",
      "\n",
      "======================================================================\n",
      "TEST 2: CHANGING INPUT QUERY\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81d1077ab58412dba9ea9be1e9a9289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476a0528bad24403bfd5f5b3e987112f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Query Score:   0.833\n",
      "Irrelevant Query Score: 0.500\n",
      "Impact: 0.333 difference\n",
      "‚úÖ HUGE IMPACT - Input Query is PRIMARY dependency\n",
      "\n",
      "======================================================================\n",
      "TEST 3: CHANGING EXPECTED OUTPUT\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda4e6593a614451b35088631574dcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc1c88a24d04e779dfa62a2fdc24dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Expected Score: 0.833\n",
      "Minimal Expected Score:  0.833\n",
      "Impact: 0.000 difference\n",
      "‚ö†Ô∏è  MODERATE IMPACT - Expected Output refines relevance judgment\n",
      "\n",
      "======================================================================\n",
      "TEST 4: CHANGING ACTUAL OUTPUT\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a24f329ae041eb869bfac3edde51ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2ef7ff1a684b14bb95ae0af6067171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Exception in callback Task.__step()\n",
       "handle: &lt;Handle Task.__step()&gt;\n",
       "Traceback (most recent call last):\n",
       "  File \n",
       "\"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py\n",
       "\", line 89, in _run\n",
       "    self._context.run(self._callback, *self._args)\n",
       "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "RuntimeError: cannot enter context: &lt;_contextvars.Context object at 0x107ae2280&gt; is already entered\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Exception in callback Task.__step()\n",
       "handle: <Handle Task.__step()>\n",
       "Traceback (most recent call last):\n",
       "  File \n",
       "\"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py\n",
       "\", line 89, in _run\n",
       "    self._context.run(self._callback, *self._args)\n",
       "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "RuntimeError: cannot enter context: <_contextvars.Context object at 0x107ae2280> is already entered\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Actual Output Score: 0.833\n",
      "Bad Actual Output Score:  0.833\n",
      "No Actual Output Score:   0.833\n",
      "Impact: 0.000 difference\n",
      "‚úÖ MINIMAL/ZERO IMPACT - Actual Output doesn't affect precision\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: DEPENDENCY RANKING\n",
      "======================================================================\n",
      "1. Retrieved Context  ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - PRIMARY (Direct subject of evaluation)\n",
      "2. Input Query        ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - PRIMARY (Defines relevance)\n",
      "3. Expected Output    ‚≠ê‚≠ê‚≠ê‚≠ê  - SECONDARY (Refines relevance)\n",
      "4. Actual Output      ‚≠ê      - MINIMAL (Not used in calculation)\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMONSTRATING CONTEXTUAL PRECISION DEPENDENCIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Base scenario\n",
    "base_query = \"What are the key features of Python?\"\n",
    "base_expected = \"Python is high-level, interpreted, and has simple syntax\"\n",
    "base_context = [\n",
    "    \"Python is a high-level programming language\",\n",
    "    \"Java is object-oriented\",\n",
    "    \"Python is interpreted, not compiled\"\n",
    "]\n",
    "\n",
    "# Test 1: Change RETRIEVED CONTEXT (Primary Dependency)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 1: CHANGING RETRIEVED CONTEXT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "context_good = [\n",
    "    \"Python is a high-level programming language\",\n",
    "    \"Python is interpreted, not compiled\",\n",
    "    \"Python has simple and readable syntax\"\n",
    "]\n",
    "\n",
    "context_bad = [\n",
    "    \"Java is object-oriented\",\n",
    "    \"JavaScript runs in browsers\",\n",
    "    \"C++ requires memory management\"\n",
    "]\n",
    "\n",
    "test_good_context = LLMTestCase(\n",
    "    input=base_query,\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=context_good\n",
    ")\n",
    "\n",
    "test_bad_context = LLMTestCase(\n",
    "    input=base_query,\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=context_bad\n",
    ")\n",
    "\n",
    "metric = ContextualPrecisionMetric(threshold=0.5, model=\"gpt-4o\")\n",
    "\n",
    "metric.measure(test_good_context)\n",
    "score_good_context = metric.score\n",
    "\n",
    "metric.measure(test_bad_context)\n",
    "score_bad_context = metric.score\n",
    "\n",
    "print(f\"Good Context Score: {score_good_context:.3f}\")\n",
    "print(f\"Bad Context Score:  {score_bad_context:.3f}\")\n",
    "print(f\"Impact: {abs(score_good_context - score_bad_context):.3f} difference\")\n",
    "print(\"‚úÖ HUGE IMPACT - Retrieved Context is PRIMARY dependency\")\n",
    "\n",
    "# Test 2: Change INPUT QUERY (Primary Dependency)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 2: CHANGING INPUT QUERY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "query_relevant = \"What are the key features of Python?\"\n",
    "query_irrelevant = \"What are the advantages of Java?\"\n",
    "\n",
    "test_relevant_query = LLMTestCase(\n",
    "    input=query_relevant,\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=base_context\n",
    ")\n",
    "\n",
    "test_irrelevant_query = LLMTestCase(\n",
    "    input=query_irrelevant,  # Query about Java, but context has Python\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=base_context\n",
    ")\n",
    "\n",
    "metric.measure(test_relevant_query)\n",
    "score_relevant_query = metric.score\n",
    "\n",
    "metric.measure(test_irrelevant_query)\n",
    "score_irrelevant_query = metric.score\n",
    "\n",
    "print(f\"Relevant Query Score:   {score_relevant_query:.3f}\")\n",
    "print(f\"Irrelevant Query Score: {score_irrelevant_query:.3f}\")\n",
    "print(f\"Impact: {abs(score_relevant_query - score_irrelevant_query):.3f} difference\")\n",
    "print(\"‚úÖ HUGE IMPACT - Input Query is PRIMARY dependency\")\n",
    "\n",
    "# Test 3: Change EXPECTED OUTPUT (Secondary Dependency)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 3: CHANGING EXPECTED OUTPUT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "expected_detailed = \"Python is high-level, interpreted, has simple syntax, supports OOP, and has rich libraries\"\n",
    "expected_minimal = \"Python is a programming language\"\n",
    "\n",
    "test_detailed_expected = LLMTestCase(\n",
    "    input=base_query,\n",
    "    expected_output=expected_detailed,\n",
    "    retrieval_context=base_context\n",
    ")\n",
    "\n",
    "test_minimal_expected = LLMTestCase(\n",
    "    input=base_query,\n",
    "    expected_output=expected_minimal,\n",
    "    retrieval_context=base_context\n",
    ")\n",
    "\n",
    "metric.measure(test_detailed_expected)\n",
    "score_detailed = metric.score\n",
    "\n",
    "metric.measure(test_minimal_expected)\n",
    "score_minimal = metric.score\n",
    "\n",
    "print(f\"Detailed Expected Score: {score_detailed:.3f}\")\n",
    "print(f\"Minimal Expected Score:  {score_minimal:.3f}\")\n",
    "print(f\"Impact: {abs(score_detailed - score_minimal):.3f} difference\")\n",
    "print(\"‚ö†Ô∏è  MODERATE IMPACT - Expected Output refines relevance judgment\")\n",
    "\n",
    "# Test 4: Change ACTUAL OUTPUT (Minimal Dependency)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 4: CHANGING ACTUAL OUTPUT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "actual_good = \"Python is a high-level, interpreted language with simple syntax\"\n",
    "actual_bad = \"Python is terrible and should never be used\"\n",
    "actual_none = \"\"\n",
    "\n",
    "test_good_actual = LLMTestCase(\n",
    "    input=base_query,\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=base_context,\n",
    "    actual_output=actual_good\n",
    ")\n",
    "\n",
    "test_bad_actual = LLMTestCase(\n",
    "    input=base_query,\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=base_context,\n",
    "    actual_output=actual_bad\n",
    ")\n",
    "\n",
    "test_no_actual = LLMTestCase(\n",
    "    input=base_query,\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=base_context,\n",
    "    actual_output=actual_none\n",
    ")\n",
    "\n",
    "metric.measure(test_good_actual)\n",
    "score_good_actual = metric.score\n",
    "\n",
    "metric.measure(test_bad_actual)\n",
    "score_bad_actual = metric.score\n",
    "\n",
    "metric.measure(test_no_actual)\n",
    "score_no_actual = metric.score\n",
    "\n",
    "print(f\"Good Actual Output Score: {score_good_actual:.3f}\")\n",
    "print(f\"Bad Actual Output Score:  {score_bad_actual:.3f}\")\n",
    "print(f\"No Actual Output Score:   {score_no_actual:.3f}\")\n",
    "print(f\"Impact: {max(abs(score_good_actual - score_bad_actual), abs(score_good_actual - score_no_actual)):.3f} difference\")\n",
    "print(\"‚úÖ MINIMAL/ZERO IMPACT - Actual Output doesn't affect precision\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: DEPENDENCY RANKING\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Retrieved Context  ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - PRIMARY (Direct subject of evaluation)\")\n",
    "print(\"2. Input Query        ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - PRIMARY (Defines relevance)\")\n",
    "print(\"3. Expected Output    ‚≠ê‚≠ê‚≠ê‚≠ê  - SECONDARY (Refines relevance)\")\n",
    "print(\"4. Actual Output      ‚≠ê      - MINIMAL (Not used in calculation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c7e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
