{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3df07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455255a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63d7135",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c000f",
   "metadata": {},
   "source": [
    "## Example 1: Basic Contextual Recall Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "753ef188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "# Define the test case\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What is the largest city in France and what river flows through it?\",\n",
    "    \n",
    "    # The actual output from your RAG system (optional for contextual recall)\n",
    "    actual_output=\"The largest city in France is Paris.\",\n",
    "    \n",
    "    # Ground truth - what the complete answer should be\n",
    "    expected_output=\"The largest city in France is Paris, and the Seine river flows through it.\",\n",
    "    \n",
    "    # Retrieved context from your retrieval system\n",
    "    retrieval_context=[\n",
    "        \"The largest city and capital of France is Paris. It is known for the Eiffel Tower.\",\n",
    "        \"Croissants are tasty French pastries popular in France.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the metric\n",
    "contextual_recall_metric = ContextualRecallMetric(\n",
    "    threshold=0.7,  # Minimum acceptable score\n",
    "    model=\"gpt-4o\",  # LLM model to use for evaluation (can use gpt-3.5-turbo, gpt-4, etc.)\n",
    "    include_reason=True,  # Include reasoning in the output\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf0ed89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b005bd7951d64b15a4cd23735e3237c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Contextual Recall Verbose Logs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Contextual Recall Verbose Logs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence 'The largest city in France is Paris' can be attributed to the 1st node in the \n",
       "retrieval context: 'The largest city and capital of France is Paris...'.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"no\",\n",
       "        \"reason\": \"The sentence 'and the Seine river flows through it' cannot be attributed to any part of the \n",
       "retrieval context, as there is no mention of the Seine river in the provided nodes.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 0.5\n",
       "Reason: The score is 0.50 because the retrieval context successfully supports the statement about Paris being the \n",
       "largest city in France, as seen in node 1. However, it lacks information regarding the Seine river, which is \n",
       "crucial for fully supporting the expected output.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence 'The largest city in France is Paris' can be attributed to the 1st node in the \n",
       "retrieval context: 'The largest city and capital of France is Paris...'.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"no\",\n",
       "        \"reason\": \"The sentence 'and the Seine river flows through it' cannot be attributed to any part of the \n",
       "retrieval context, as there is no mention of the Seine river in the provided nodes.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 0.5\n",
       "Reason: The score is 0.50 because the retrieval context successfully supports the statement about Paris being the \n",
       "largest city in France, as seen in node 1. However, it lacks information regarding the Seine river, which is \n",
       "crucial for fully supporting the expected output.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "======================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Recall Score: 0.5\n",
      "\n",
      "Reason:\n",
      "The score is 0.50 because the retrieval context successfully supports the statement about Paris being the largest city in France, as seen in node 1. However, it lacks information regarding the Seine river, which is crucial for fully supporting the expected output.\n",
      "\n",
      "Success (>= threshold): False\n"
     ]
    }
   ],
   "source": [
    "# Measure the metric\n",
    "try:\n",
    "    contextual_recall_metric.measure(test_case)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Contextual Recall Score: {contextual_recall_metric.score}\")\n",
    "    print(f\"\\nReason:\\n{contextual_recall_metric.reason}\")\n",
    "    print(f\"\\nSuccess (>= threshold): {contextual_recall_metric.is_successful()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error measuring contextual recall: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4ad53",
   "metadata": {},
   "source": [
    "## Example 2: Complete Contextual Recall (High Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e90f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f663b997ed94801a70e455aae0cf20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Contextual Recall Verbose Logs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Contextual Recall Verbose Logs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence can be attributed to the 1st node: 'Paris is the largest city and capital of \n",
       "France...' and the 2nd node: 'The Seine river flows through Paris...'.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 1.0\n",
       "Reason: The score is 1.00 because the expected output perfectly aligns with the nodes in the retrieval context, \n",
       "accurately reflecting the information about Paris and the Seine river.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence can be attributed to the 1st node: 'Paris is the largest city and capital of \n",
       "France...' and the 2nd node: 'The Seine river flows through Paris...'.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 1.0\n",
       "Reason: The score is 1.00 because the expected output perfectly aligns with the nodes in the retrieval context, \n",
       "accurately reflecting the information about Paris and the Seine river.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "======================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Recall Score: 1.0\n",
      "\n",
      "Reason:\n",
      "The score is 1.00 because the expected output perfectly aligns with the nodes in the retrieval context, accurately reflecting the information about Paris and the Seine river.\n",
      "\n",
      "Success (>= threshold): True\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Test case with complete information in context\n",
    "test_case_complete = LLMTestCase(\n",
    "    input=\"What is the largest city in France and what river flows through it?\",\n",
    "    \n",
    "    actual_output=\"The largest city in France is Paris, and the Seine river flows through it.\",\n",
    "    \n",
    "    expected_output=\"The largest city in France is Paris, and the Seine river flows through it.\",\n",
    "    \n",
    "    retrieval_context=[\n",
    "        \"Paris is the largest city and capital of France, located in the north-central part of the country.\", \n",
    "        \"The Seine river flows through Paris, dividing the city into the Left Bank and Right Bank.\", \n",
    "        \"Paris is known for landmarks like the Eiffel Tower and the Louvre Museum.\" ] )\n",
    "\n",
    "# Initialize the metric\n",
    "contextual_recall_metric = ContextualRecallMetric(\n",
    "    threshold=0.7, \n",
    "    model=\"gpt-4o\", \n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "# Measure the metric\n",
    "try:\n",
    "    contextual_recall_metric.measure(test_case_complete)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Contextual Recall Score: {contextual_recall_metric.score}\") \n",
    "    print(f\"\\nReason:\\n{contextual_recall_metric.reason}\") \n",
    "    print(f\"\\nSuccess (>= threshold): {contextual_recall_metric.is_successful()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error measuring contextual recall: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415af220",
   "metadata": {},
   "source": [
    "## Example 3: Medical Query with Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff926ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3338cd7dec442689cd5e6331e411b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Contextual Recall Verbose Logs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Contextual Recall Verbose Logs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence matches the 1st node: 'Type 2 Diabetes symptoms include increased thirst, frequent \n",
       "urination, and fatigue.' and the 2nd node: 'Patients may experience blurred vision...'.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence aligns with the 3rd node: 'Treatment involves lifestyle changes including a healthy\n",
       "diet and regular exercise.', the 4th node: 'Metformin is a common oral medication...', and the 5th node: 'In \n",
       "advanced cases, insulin therapy may be required...'.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"no\",\n",
       "        \"reason\": \"The sentence about unexplained weight loss is not mentioned in any of the nodes in the retrieval\n",
       "context.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 0.6666666666666666\n",
       "Reason: The score is 0.67 because most of the expected output aligns well with the nodes in the retrieval context. \n",
       "Sentences about symptoms and treatment options are supported by nodes 1 through 5. However, the mention of \n",
       "unexplained weight loss lacks support from any node, affecting the overall score.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence matches the 1st node: 'Type 2 Diabetes symptoms include increased thirst, frequent \n",
       "urination, and fatigue.' and the 2nd node: 'Patients may experience blurred vision...'.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence aligns with the 3rd node: 'Treatment involves lifestyle changes including a healthy\n",
       "diet and regular exercise.', the 4th node: 'Metformin is a common oral medication...', and the 5th node: 'In \n",
       "advanced cases, insulin therapy may be required...'.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"no\",\n",
       "        \"reason\": \"The sentence about unexplained weight loss is not mentioned in any of the nodes in the retrieval\n",
       "context.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 0.6666666666666666\n",
       "Reason: The score is 0.67 because most of the expected output aligns well with the nodes in the retrieval context. \n",
       "Sentences about symptoms and treatment options are supported by nodes 1 through 5. However, the mention of \n",
       "unexplained weight loss lacks support from any node, affecting the overall score.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "======================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MEDICAL QUERY EVALUATION\n",
      "============================================================\n",
      "Contextual Recall Score: 0.67\n",
      "\n",
      "Reason:\n",
      "The score is 0.67 because most of the expected output aligns well with the nodes in the retrieval context. Sentences about symptoms and treatment options are supported by nodes 1 through 5. However, the mention of unexplained weight loss lacks support from any node, affecting the overall score.\n",
      "\n",
      "Passed Threshold (0.8): False\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Medical information retrieval scenario\n",
    "test_case_medical = LLMTestCase(\n",
    "    input=\"What are the symptoms and treatment options for Type 2 Diabetes?\",\n",
    "    \n",
    "    actual_output=\"\"\"Type 2 Diabetes symptoms include increased thirst, frequent urination, \n",
    "    fatigue, and blurred vision. Treatment includes lifestyle changes like diet and exercise, \n",
    "    oral medications like Metformin, and insulin therapy in advanced cases.\"\"\",\n",
    "    \n",
    "    expected_output=\"\"\"Type 2 Diabetes presents with symptoms including increased thirst, \n",
    "    frequent urination, fatigue, and blurred vision. Treatment \n",
    "    options include lifestyle modifications (diet and exercise), oral medications such as \n",
    "    Metformin, and insulin therapy for severe cases. This also cause unexplained weight loss\"\"\",\n",
    "    \n",
    "    retrieval_context=[\n",
    "        \"Type 2 Diabetes symptoms include increased thirst (polydipsia), frequent urination (polyuria), and fatigue.\",\n",
    "        \"Patients may experience blurred vision and slow-healing wounds.\",\n",
    "        \"Treatment involves lifestyle changes including a healthy diet and regular exercise.\",\n",
    "        \"Metformin is a common oral medication prescribed for Type 2 Diabetes management.\",\n",
    "        \"In advanced cases, insulin therapy may be required to control blood sugar levels.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the metric\n",
    "metric = ContextualRecallMetric(\n",
    "    threshold=0.8, \n",
    "    model=\"gpt-4oo\", \n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "# Measure contextual recall\n",
    "try:\n",
    "    metric.measure(test_case_medical)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MEDICAL QUERY EVALUATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Contextual Recall Score: {metric.score:.2f}\")\n",
    "    print(f\"\\nReason:\\n{metric.reason}\")\n",
    "    print(f\"\\nPassed Threshold (0.8): {metric.is_successful()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error measuring contextual recall: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6939282",
   "metadata": {},
   "source": [
    "## Example 4: Batch Evaluation with Multiple Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b16e5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Recall Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Recall Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c83360fa9844908f8dd090d8c96c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Contextual Recall (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the expected output perfectly aligns with the information in the 1st node in the retrieval context, confirming the historical fact with precision. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Who invented the telephone?\n",
      "  - actual output: None\n",
      "  - expected output: Alexander Graham Bell invented the telephone in 1876.\n",
      "  - context: None\n",
      "  - retrieval context: ['Alexander Graham Bell is credited with inventing the telephone.', 'The telephone revolutionized communication in the 19th century.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Contextual Recall (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the expected output perfectly aligns with the 1st node in the retrieval context, showcasing a flawless match. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What are the primary colors?\n",
      "  - actual output: None\n",
      "  - expected output: The primary colors are red, blue, and yellow.\n",
      "  - context: None\n",
      "  - retrieval context: ['Primary colors are red, blue, and yellow.', 'Secondary colors are created by mixing primary colors.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Contextual Recall (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the expected output perfectly aligns with the information in the 1st node in the retrieval context, confirming the capital of Japan as Tokyo. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the capital of Japan?\n",
      "  - actual output: None\n",
      "  - expected output: The capital of Japan is Tokyo.\n",
      "  - context: None\n",
      "  - retrieval context: ['Tokyo is the capital and largest city of Japan.', 'Japan is an island nation in East Asia.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Contextual Recall: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">‚ö† WARNING:</span> No hyperparameters logged.\n",
       "¬ª <a href=\"https://deepeval.com/docs/evaluation-prompts\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Log hyperparameters</span></a> to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33m‚ö† WARNING:\u001b[0m No hyperparameters logged.\n",
       "¬ª \u001b]8;id=369110;https://deepeval.com/docs/evaluation-prompts\u001b\\\u001b[1;34mLog hyperparameters\u001b[0m\u001b]8;;\u001b\\ to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Evaluation completed üéâ! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.</span>06s | token cost: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00884</span> USD<span style=\"font-weight: bold\">)</span>\n",
       "¬ª Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
       "   ¬ª Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0</span>\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Evaluation completed üéâ! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m3.\u001b[0m06s | token cost: \u001b[1;36m0.00884\u001b[0m USD\u001b[1m)\u001b[0m\n",
       "¬ª Test Results \u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
       "   ¬ª Pass Rate: \u001b[1;36m100.0\u001b[0m% | Passed: \u001b[1;32m3\u001b[0m | Failed: \u001b[1;31m0\u001b[0m\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BATCH EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Test Case 1:\n",
      "  Query: What is the capital of Japan?\n",
      "  Contextual Recall: 1.00\n",
      "  Passed: True\n",
      "\n",
      "Test Case 2:\n",
      "  Query: Who invented the telephone?\n",
      "  Contextual Recall: 1.00\n",
      "  Passed: True\n",
      "\n",
      "Test Case 3:\n",
      "  Query: What are the primary colors?\n",
      "  Contextual Recall: 1.00\n",
      "  Passed: True\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Create multiple test cases\n",
    "test_cases = [\n",
    "    LLMTestCase(\n",
    "        input=\"What is the capital of Japan?\",\n",
    "        expected_output=\"The capital of Japan is Tokyo.\",\n",
    "        retrieval_context=[\n",
    "            \"Tokyo is the capital and largest city of Japan.\",\n",
    "            \"Japan is an island nation in East Asia.\"\n",
    "        ]\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"Who invented the telephone?\",\n",
    "        expected_output=\"Alexander Graham Bell invented the telephone in 1876.\",\n",
    "        retrieval_context=[\n",
    "            \"Alexander Graham Bell is credited with inventing the telephone.\",\n",
    "            \"The telephone revolutionized communication in the 19th century.\"\n",
    "        ]\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"What are the primary colors?\",\n",
    "        expected_output=\"The primary colors are red, blue, and yellow.\",\n",
    "        retrieval_context=[\n",
    "            \"Primary colors are red, blue, and yellow.\",\n",
    "            \"Secondary colors are created by mixing primary colors.\"\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define metric\n",
    "contextual_recall = ContextualRecallMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "# Batch evaluate\n",
    "results = evaluate(\n",
    "    test_cases=test_cases,\n",
    "    metrics=[contextual_recall]\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BATCH EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\nTest Case {i}:\")\n",
    "    print(f\"  Query: {test_case.input}\")\n",
    "    print(f\"  Contextual Recall: {results.test_results[i-1].metrics_data[0].score:.2f}\")\n",
    "    print(f\"  Passed: {results.test_results[i-1].metrics_data[0].success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df691153",
   "metadata": {},
   "source": [
    "## Example 5: Real-World RAG Pipeline Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c6e4482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2e51d88eb5486bbb8b30162b7d26a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Contextual Recall Verbose Logs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Contextual Recall Verbose Logs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence can be attributed to the 1st node in the retrieval context: 'Python is a high-level\n",
       "programming language known for its simplicity.'...\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence can be attributed to the 2nd node in the retrieval context: 'Python supports \n",
       "multiple programming paradigms including OOP and functional.'...\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence can be attributed to the 3rd node in the retrieval context: 'Python has extensive \n",
       "libraries for data science like NumPy and Pandas.'...\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 1.0\n",
       "Reason: The score is 1.00 because every sentence in the expected output is perfectly aligned with the nodes in the \n",
       "retrieval context, showcasing a comprehensive understanding of Python's features. Great job!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence can be attributed to the 1st node in the retrieval context: 'Python is a high-level\n",
       "programming language known for its simplicity.'...\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence can be attributed to the 2nd node in the retrieval context: 'Python supports \n",
       "multiple programming paradigms including OOP and functional.'...\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence can be attributed to the 3rd node in the retrieval context: 'Python has extensive \n",
       "libraries for data science like NumPy and Pandas.'...\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 1.0\n",
       "Reason: The score is 1.00 because every sentence in the expected output is perfectly aligned with the nodes in the \n",
       "retrieval context, showcasing a comprehensive understanding of Python's features. Great job!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "======================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAG PIPELINE EVALUATION\n",
      "============================================================\n",
      "Query: What are the key features of Python?\n",
      "\n",
      "Generated Answer:\n",
      "Python is a high-level language with simple syntax and supports multiple paradigms.\n",
      "\n",
      "Expected Answer:\n",
      "Python is a high-level programming language with simple syntax, \n",
      "supports multiple programming paradigms including object-oriented and functional programming, \n",
      "and has extensive libraries for various applications including data science.\n",
      "\n",
      "Retrieved Context:\n",
      "  1. Python is a high-level programming language known for its simplicity.\n",
      "  2. Python supports multiple programming paradigms including OOP and functional.\n",
      "  3. Python has extensive libraries for data science like NumPy and Pandas.\n",
      "\n",
      "Contextual Recall Score: 1.00\n",
      "Reason: The score is 1.00 because every sentence in the expected output is perfectly aligned with the nodes in the retrieval context, showcasing a comprehensive understanding of Python's features. Great job!\n",
      "Passed: True\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import openai\n",
    "\n",
    "# Simulate a RAG pipeline function\n",
    "def rag_pipeline(query: str):\n",
    "    \"\"\"\n",
    "    Simulated RAG pipeline that:\n",
    "    1. Retrieves relevant documents\n",
    "    2. Generates an answer\n",
    "    \"\"\"\n",
    "    # Simulated retrieval (in real scenario, this would be vector search)\n",
    "    if \"python\" in query.lower():\n",
    "        retrieved_docs = [\n",
    "            \"Python is a high-level programming language known for its simplicity.\",\n",
    "            \"Python supports multiple programming paradigms including OOP and functional.\",\n",
    "            \"Python has extensive libraries for data science like NumPy and Pandas.\"\n",
    "        ]\n",
    "    else:\n",
    "        retrieved_docs = [\"No relevant information found.\"]\n",
    "    \n",
    "    # Simulated generation (in real scenario, this would be LLM generation)\n",
    "    generated_answer = \"Python is a high-level language with simple syntax and supports multiple paradigms.\"\n",
    "    \n",
    "    return {\n",
    "        \"answer\": generated_answer,\n",
    "        \"context\": retrieved_docs\n",
    "    }\n",
    "\n",
    "# Query to test\n",
    "query = \"What are the key features of Python?\"\n",
    "ground_truth = \"\"\"Python is a high-level programming language with simple syntax, \n",
    "supports multiple programming paradigms including object-oriented and functional programming, \n",
    "and has extensive libraries for various applications including data science.\"\"\"\n",
    "\n",
    "# Run RAG pipeline\n",
    "rag_result = rag_pipeline(query)\n",
    "\n",
    "# Create test case\n",
    "test_case = LLMTestCase(\n",
    "    input=query,\n",
    "    actual_output=rag_result[\"answer\"],\n",
    "    expected_output=ground_truth,\n",
    "    retrieval_context=rag_result[\"context\"]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "metric = ContextualRecallMetric(threshold=0.75, model=\"gpt-4o\", include_reason=True, verbose_mode=True)\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RAG PIPELINE EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nGenerated Answer:\\n{rag_result['answer']}\")\n",
    "print(f\"\\nExpected Answer:\\n{ground_truth}\")\n",
    "print(f\"\\nRetrieved Context:\")\n",
    "for i, doc in enumerate(rag_result['context'], 1):\n",
    "    print(f\"  {i}. {doc}\")\n",
    "print(f\"\\nContextual Recall Score: {metric.score:.2f}\")\n",
    "print(f\"Reason: {metric.reason}\")\n",
    "print(f\"Passed: {metric.is_successful()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3c4de",
   "metadata": {},
   "source": [
    "## Example 7: Understanding LLM-Based Evaluation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76283fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb0a833424a407aa225657e92041f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Contextual Recall Verbose Logs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Contextual Recall Verbose Logs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The 1st node mentions 'Burning fossil fuels like coal, oil, and gas releases carbon dioxide into\n",
       "the atmosphere', which aligns with 'burning fossil fuels' causing greenhouse gas emissions.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The 2nd node states 'Deforestation reduces the Earth's capacity to absorb CO2', supporting the \n",
       "claim that deforestation contributes to climate change.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The 3rd node mentions 'Industrial processes and agriculture contribute methane emissions', which\n",
       "corresponds to 'industrial activities' increasing methane levels.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 1.0\n",
       "Reason: The score is 1.00 because every sentence in the expected output is fully supported by the nodes in the \n",
       "retrieval context, with no unsupportive reasons present. Great job!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The 1st node mentions 'Burning fossil fuels like coal, oil, and gas releases carbon dioxide into\n",
       "the atmosphere', which aligns with 'burning fossil fuels' causing greenhouse gas emissions.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The 2nd node states 'Deforestation reduces the Earth's capacity to absorb CO2', supporting the \n",
       "claim that deforestation contributes to climate change.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The 3rd node mentions 'Industrial processes and agriculture contribute methane emissions', which\n",
       "corresponds to 'industrial activities' increasing methane levels.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 1.0\n",
       "Reason: The score is 1.00 because every sentence in the expected output is fully supported by the nodes in the \n",
       "retrieval context, with no unsupportive reasons present. Great job!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "======================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LLM-BASED EVALUATION BREAKDOWN\n",
      "============================================================\n",
      "\n",
      "Expected Output Analysis:\n",
      "  Statement 1: 'Climate change caused by greenhouse gas emissions'\n",
      "  Statement 2: 'Emissions from burning fossil fuels'\n",
      "  Statement 3: 'Deforestation contributes to climate change'\n",
      "  Statement 4: 'Industrial activities increase emissions'\n",
      "  Statement 5: 'CO2 and methane levels increase'\n",
      "\n",
      "Attribution Check (LLM verifies each):\n",
      "  ‚úì Statement 1: Found in context (fossil fuels release CO2)\n",
      "  ‚úì Statement 2: Found in context (burning fossil fuels)\n",
      "  ‚úì Statement 3: Found in context (deforestation reduces CO2 absorption)\n",
      "  ‚úì Statement 4: Found in context (industrial processes contribute)\n",
      "  ‚úì Statement 5: Found in context (methane from agriculture)\n",
      "\n",
      "Final Score: 1.00\n",
      "Calculation: 5 attributable statements / 5 total statements = 1.0\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Test case demonstrating how DeepEval uses LLM for evaluation\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What causes climate change?\",\n",
    "    expected_output=\"\"\"Climate change is primarily caused by greenhouse gas emissions \n",
    "    from burning fossil fuels, deforestation, and industrial activities. These activities \n",
    "    increase CO2 and methane levels in the atmosphere.\"\"\",\n",
    "    retrieval_context=[\n",
    "        \"Burning fossil fuels like coal, oil, and gas releases carbon dioxide into the atmosphere.\",\n",
    "        \"Deforestation reduces the Earth's capacity to absorb CO2.\",\n",
    "        \"Industrial processes and agriculture contribute methane emissions.\",\n",
    "        \"The greenhouse effect traps heat in the atmosphere, causing global warming.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create metric with verbose output\n",
    "metric = ContextualRecallMetric(\n",
    "    threshold=0.8,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    "    verbose_mode=True  # Enable verbose logging\n",
    ")\n",
    "\n",
    "# Measure\n",
    "metric.measure(test_case)\n",
    "\n",
    "# DeepEval internally does this:\n",
    "# 1. Extracts key statements from expected_output using LLM\n",
    "# 2. For each statement, asks LLM: \"Can this statement be attributed to the retrieval context?\"\n",
    "# 3. Counts attributable statements\n",
    "# 4. Calculates score = attributable_count / total_statements\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LLM-BASED EVALUATION BREAKDOWN\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nExpected Output Analysis:\")\n",
    "print(\"  Statement 1: 'Climate change caused by greenhouse gas emissions'\")\n",
    "print(\"  Statement 2: 'Emissions from burning fossil fuels'\")\n",
    "print(\"  Statement 3: 'Deforestation contributes to climate change'\")\n",
    "print(\"  Statement 4: 'Industrial activities increase emissions'\")\n",
    "print(\"  Statement 5: 'CO2 and methane levels increase'\")\n",
    "\n",
    "print(f\"\\nAttribution Check (LLM verifies each):\")\n",
    "print(\"  ‚úì Statement 1: Found in context (fossil fuels release CO2)\")\n",
    "print(\"  ‚úì Statement 2: Found in context (burning fossil fuels)\")\n",
    "print(\"  ‚úì Statement 3: Found in context (deforestation reduces CO2 absorption)\")\n",
    "print(\"  ‚úì Statement 4: Found in context (industrial processes contribute)\")\n",
    "print(\"  ‚úì Statement 5: Found in context (methane from agriculture)\")\n",
    "\n",
    "print(f\"\\nFinal Score: {metric.score:.2f}\")\n",
    "print(f\"Calculation: 5 attributable statements / 5 total statements = 1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12464e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
