{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b4e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq langchain==0.3.11\n",
    "!pip install -qq langchain-openai==0.2.12\n",
    "!pip install -qq langchain-community==0.3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b890d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.76.0 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qq ragas==0.2.8\n",
    "!pip install -qq deepeval==3.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c3df07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "455255a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "## Example 1: Basic Contextual Recall Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125bac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c000f",
   "metadata": {},
   "source": [
    "## Example 1: Basic Contextual Recall Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753ef188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "# Define the test case\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What is the largest city in France and what river flows through it?\",\n",
    "    \n",
    "    # The actual output from your RAG system (optional for contextual recall)\n",
    "    actual_output=\"The largest city in France is Paris.\",\n",
    "    \n",
    "    # Ground truth - what the complete answer should be\n",
    "    expected_output=\"The largest city in France is Paris, and the Seine river flows through it.\",\n",
    "    \n",
    "    # Retrieved context from your retrieval system\n",
    "    retrieval_context=[\n",
    "        \"The largest city and capital of France is Paris. It is known for the Eiffel Tower.\",\n",
    "        \"Croissants are tasty French pastries popular in France.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the metric\n",
    "contextual_recall_metric = ContextualRecallMetric(\n",
    "    threshold=0.7,  # Minimum acceptable score\n",
    "    model=\"gpt-4o\",  # LLM model to use for evaluation (can use gpt-3.5-turbo, gpt-4, etc.)\n",
    "    include_reason=True,  # Include reasoning in the output\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf0ed89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/sourav.banerjee/Library/Python/3.9/lib/python/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/sourav.banerjee/Library/Python/3.9/lib/python/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Contextual Recall Verbose Logs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Contextual Recall Verbose Logs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence 'The largest city in France is Paris' can be attributed to the 1st node in the \n",
       "retrieval context: 'The largest city and capital of France is Paris...'.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"no\",\n",
       "        \"reason\": \"The sentence 'and the Seine river flows through it' cannot be attributed to any part of the \n",
       "retrieval context, as there is no mention of the Seine river in the provided nodes.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 0.5\n",
       "Reason: The score is 0.50 because the sentence 'The largest city in France is Paris' aligns well with the 1st node \n",
       "in the retrieval context, confirming its accuracy. However, the sentence 'and the Seine river flows through it' \n",
       "lacks support from any node in the retrieval context, leading to a partial match.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence 'The largest city in France is Paris' can be attributed to the 1st node in the \n",
       "retrieval context: 'The largest city and capital of France is Paris...'.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"no\",\n",
       "        \"reason\": \"The sentence 'and the Seine river flows through it' cannot be attributed to any part of the \n",
       "retrieval context, as there is no mention of the Seine river in the provided nodes.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 0.5\n",
       "Reason: The score is 0.50 because the sentence 'The largest city in France is Paris' aligns well with the 1st node \n",
       "in the retrieval context, confirming its accuracy. However, the sentence 'and the Seine river flows through it' \n",
       "lacks support from any node in the retrieval context, leading to a partial match.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "======================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Recall Score: 0.5\n",
      "\n",
      "Reason:\n",
      "The score is 0.50 because the sentence 'The largest city in France is Paris' aligns well with the 1st node in the retrieval context, confirming its accuracy. However, the sentence 'and the Seine river flows through it' lacks support from any node in the retrieval context, leading to a partial match.\n",
      "\n",
      "Success (>= threshold): False\n"
     ]
    }
   ],
   "source": [
    "# Measure the metric\n",
    "try:\n",
    "    contextual_recall_metric.measure(test_case)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Contextual Recall Score: {contextual_recall_metric.score}\")\n",
    "    print(f\"\\nReason:\\n{contextual_recall_metric.reason}\")\n",
    "    print(f\"\\nSuccess (>= threshold): {contextual_recall_metric.is_successful()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error measuring contextual recall: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4ad53",
   "metadata": {},
   "source": [
    "## Example 2: Complete Contextual Recall (High Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e90f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Test case with complete information in context\n",
    "test_case_complete = LLMTestCase(\n",
    "    input=\"What is the largest city in France and what river flows through it?\",\n",
    "    \n",
    "    actual_output=\"The largest city in France is Paris, and the Seine river flows through it.\",\n",
    "    \n",
    "    expected_output=\"The largest city in France is Paris, and the Seine river flows through it.\",\n",
    "    \n",
    "    retrieval_context=[\n",
    "        \"Paris is the largest city and capital of France, located in the north-central part of the country.\", \n",
    "        \"The Seine river flows through Paris, dividing the city into the Left Bank and Right Bank.\", \n",
    "        \"Paris is known for landmarks like the Eiffel Tower and the Louvre Museum.\" ] )\n",
    "\n",
    "# Initialize the metric\n",
    "contextual_recall_metric = ContextualRecallMetric(\n",
    "    threshold=0.7, \n",
    "    model=\"gpt-4o\", \n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "# Measure the metric\n",
    "try:\n",
    "    contextual_recall_metric.measure(test_case_complete)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Contextual Recall Score: {contextual_recall_metric.score}\") \n",
    "    print(f\"\\nReason:\\n{contextual_recall_metric.reason}\") \n",
    "    print(f\"\\nSuccess (>= threshold): {contextual_recall_metric.is_successful()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error measuring contextual recall: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415af220",
   "metadata": {},
   "source": [
    "## Example 3: Medical Query with Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff926ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Medical information retrieval scenario\n",
    "test_case_medical = LLMTestCase(\n",
    "    input=\"What are the symptoms and treatment options for Type 2 Diabetes?\",\n",
    "    \n",
    "    actual_output=\"\"\"Type 2 Diabetes symptoms include increased thirst, frequent urination, \n",
    "    fatigue, and blurred vision. Treatment includes lifestyle changes like diet and exercise, \n",
    "    oral medications like Metformin, and insulin therapy in advanced cases.\"\"\",\n",
    "    \n",
    "    expected_output=\"\"\"Type 2 Diabetes presents with symptoms including increased thirst, \n",
    "    frequent urination, unexplained weight loss, fatigue, and blurred vision. Treatment \n",
    "    options include lifestyle modifications (diet and exercise), oral medications such as \n",
    "    Metformin, and insulin therapy for severe cases.\"\"\",\n",
    "    \n",
    "    retrieval_context=[\n",
    "        \"Type 2 Diabetes symptoms include increased thirst (polydipsia), frequent urination (polyuria), and fatigue.\",\n",
    "        \"Patients may experience blurred vision and slow-healing wounds.\",\n",
    "        \"Treatment involves lifestyle changes including a healthy diet and regular exercise.\",\n",
    "        \"Metformin is a common oral medication prescribed for Type 2 Diabetes management.\",\n",
    "        \"In advanced cases, insulin therapy may be required to control blood sugar levels.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the metric\n",
    "metric = ContextualRecallMetric(\n",
    "    threshold=0.8, \n",
    "    model=\"gpt-4o\", \n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "# Measure contextual recall\n",
    "try:\n",
    "    metric.measure(test_case_medical)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MEDICAL QUERY EVALUATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Contextual Recall Score: {metric.score:.2f}\")\n",
    "    print(f\"\\nReason:\\n{metric.reason}\")\n",
    "    print(f\"\\nPassed Threshold (0.8): {metric.is_successful()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error measuring contextual recall: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
