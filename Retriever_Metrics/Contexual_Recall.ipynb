{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3df07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455255a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63d7135",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c000f",
   "metadata": {},
   "source": [
    "## Example 1: Basic Contextual Recall Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "753ef188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "# Define the test case\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What is the largest city in France and what river flows through it?\",\n",
    "    \n",
    "    # The actual output from your RAG system (optional for contextual recall)\n",
    "    actual_output=\"The largest city in France is Paris.\",\n",
    "    \n",
    "    # Ground truth - what the complete answer should be\n",
    "    expected_output=\"The largest city in France is Paris, and the Seine river flows through it.\",\n",
    "    \n",
    "    # Retrieved context from your retrieval system\n",
    "    retrieval_context=[\n",
    "        \"The largest city and capital of France is Paris. It is known for the Eiffel Tower.\",\n",
    "        \"Croissants are tasty French pastries popular in France.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the metric\n",
    "contextual_recall_metric = ContextualRecallMetric(\n",
    "    threshold=0.7,  # Minimum acceptable score\n",
    "    model=\"gpt-4o\",  # LLM model to use for evaluation (can use gpt-3.5-turbo, gpt-4, etc.)\n",
    "    include_reason=True,  # Include reasoning in the output\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf0ed89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b005bd7951d64b15a4cd23735e3237c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Contextual Recall Verbose Logs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Contextual Recall Verbose Logs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence 'The largest city in France is Paris' can be attributed to the 1st node in the \n",
       "retrieval context: 'The largest city and capital of France is Paris...'.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"no\",\n",
       "        \"reason\": \"The sentence 'and the Seine river flows through it' cannot be attributed to any part of the \n",
       "retrieval context, as there is no mention of the Seine river in the provided nodes.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 0.5\n",
       "Reason: The score is 0.50 because the retrieval context successfully supports the statement about Paris being the \n",
       "largest city in France, as seen in node 1. However, it lacks information regarding the Seine river, which is \n",
       "crucial for fully supporting the expected output.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence 'The largest city in France is Paris' can be attributed to the 1st node in the \n",
       "retrieval context: 'The largest city and capital of France is Paris...'.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"no\",\n",
       "        \"reason\": \"The sentence 'and the Seine river flows through it' cannot be attributed to any part of the \n",
       "retrieval context, as there is no mention of the Seine river in the provided nodes.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 0.5\n",
       "Reason: The score is 0.50 because the retrieval context successfully supports the statement about Paris being the \n",
       "largest city in France, as seen in node 1. However, it lacks information regarding the Seine river, which is \n",
       "crucial for fully supporting the expected output.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "======================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Recall Score: 0.5\n",
      "\n",
      "Reason:\n",
      "The score is 0.50 because the retrieval context successfully supports the statement about Paris being the largest city in France, as seen in node 1. However, it lacks information regarding the Seine river, which is crucial for fully supporting the expected output.\n",
      "\n",
      "Success (>= threshold): False\n"
     ]
    }
   ],
   "source": [
    "# Measure the metric\n",
    "try:\n",
    "    contextual_recall_metric.measure(test_case)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Contextual Recall Score: {contextual_recall_metric.score}\")\n",
    "    print(f\"\\nReason:\\n{contextual_recall_metric.reason}\")\n",
    "    print(f\"\\nSuccess (>= threshold): {contextual_recall_metric.is_successful()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error measuring contextual recall: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4ad53",
   "metadata": {},
   "source": [
    "## Example 2: Complete Contextual Recall (High Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e90f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f663b997ed94801a70e455aae0cf20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Contextual Recall Verbose Logs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Contextual Recall Verbose Logs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence can be attributed to the 1st node: 'Paris is the largest city and capital of \n",
       "France...' and the 2nd node: 'The Seine river flows through Paris...'.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 1.0\n",
       "Reason: The score is 1.00 because the expected output perfectly aligns with the nodes in the retrieval context, \n",
       "accurately reflecting the information about Paris and the Seine river.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence can be attributed to the 1st node: 'Paris is the largest city and capital of \n",
       "France...' and the 2nd node: 'The Seine river flows through Paris...'.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 1.0\n",
       "Reason: The score is 1.00 because the expected output perfectly aligns with the nodes in the retrieval context, \n",
       "accurately reflecting the information about Paris and the Seine river.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "======================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Recall Score: 1.0\n",
      "\n",
      "Reason:\n",
      "The score is 1.00 because the expected output perfectly aligns with the nodes in the retrieval context, accurately reflecting the information about Paris and the Seine river.\n",
      "\n",
      "Success (>= threshold): True\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Test case with complete information in context\n",
    "test_case_complete = LLMTestCase(\n",
    "    input=\"What is the largest city in France and what river flows through it?\",\n",
    "    \n",
    "    actual_output=\"The largest city in France is Paris, and the Seine river flows through it.\",\n",
    "    \n",
    "    expected_output=\"The largest city in France is Paris, and the Seine river flows through it.\",\n",
    "    \n",
    "    retrieval_context=[\n",
    "        \"Paris is the largest city and capital of France, located in the north-central part of the country.\", \n",
    "        \"The Seine river flows through Paris, dividing the city into the Left Bank and Right Bank.\", \n",
    "        \"Paris is known for landmarks like the Eiffel Tower and the Louvre Museum.\" ] )\n",
    "\n",
    "# Initialize the metric\n",
    "contextual_recall_metric = ContextualRecallMetric(\n",
    "    threshold=0.7, \n",
    "    model=\"gpt-4o\", \n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "# Measure the metric\n",
    "try:\n",
    "    contextual_recall_metric.measure(test_case_complete)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Contextual Recall Score: {contextual_recall_metric.score}\") \n",
    "    print(f\"\\nReason:\\n{contextual_recall_metric.reason}\") \n",
    "    print(f\"\\nSuccess (>= threshold): {contextual_recall_metric.is_successful()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error measuring contextual recall: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415af220",
   "metadata": {},
   "source": [
    "## Example 3: Medical Query with Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff926ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04adf118775847faa6000ab838785ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Contextual Recall Verbose Logs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Contextual Recall Verbose Logs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence matches the 1st node: 'Type 2 Diabetes symptoms include increased thirst \n",
       "(polydipsia), frequent urination (polyuria), and fatigue.' and the 2nd node: 'Patients may experience blurred \n",
       "vision...'.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence aligns with the 3rd node: 'Treatment involves lifestyle changes including a healthy\n",
       "diet and regular exercise.', the 4th node: 'Metformin is a common oral medication...', and the 5th node: 'In \n",
       "advanced cases, insulin therapy may be required...'.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 1.0\n",
       "Reason: The score is 1.00 because every sentence in the expected output is perfectly aligned with the nodes in the \n",
       "retrieval context, covering all aspects from symptoms to treatment options. Great job!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Verdicts:\n",
       "[\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence matches the 1st node: 'Type 2 Diabetes symptoms include increased thirst \n",
       "(polydipsia), frequent urination (polyuria), and fatigue.' and the 2nd node: 'Patients may experience blurred \n",
       "vision...'.\"\n",
       "    },\n",
       "    {\n",
       "        \"verdict\": \"yes\",\n",
       "        \"reason\": \"The sentence aligns with the 3rd node: 'Treatment involves lifestyle changes including a healthy\n",
       "diet and regular exercise.', the 4th node: 'Metformin is a common oral medication...', and the 5th node: 'In \n",
       "advanced cases, insulin therapy may be required...'.\"\n",
       "    }\n",
       "]\n",
       " \n",
       "Score: 1.0\n",
       "Reason: The score is 1.00 because every sentence in the expected output is perfectly aligned with the nodes in the \n",
       "retrieval context, covering all aspects from symptoms to treatment options. Great job!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "======================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MEDICAL QUERY EVALUATION\n",
      "============================================================\n",
      "Contextual Recall Score: 1.00\n",
      "\n",
      "Reason:\n",
      "The score is 1.00 because every sentence in the expected output is perfectly aligned with the nodes in the retrieval context, covering all aspects from symptoms to treatment options. Great job!\n",
      "\n",
      "Passed Threshold (0.8): True\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Medical information retrieval scenario\n",
    "test_case_medical = LLMTestCase(\n",
    "    input=\"What are the symptoms and treatment options for Type 2 Diabetes?\",\n",
    "    \n",
    "    actual_output=\"\"\"Type 2 Diabetes symptoms include increased thirst, frequent urination, \n",
    "    fatigue, and blurred vision. Treatment includes lifestyle changes like diet and exercise, \n",
    "    oral medications like Metformin, and insulin therapy in advanced cases.\"\"\",\n",
    "    \n",
    "    expected_output=\"\"\"Type 2 Diabetes presents with symptoms including increased thirst, \n",
    "    frequent urination, unexplained weight loss, fatigue, and blurred vision. Treatment \n",
    "    options include lifestyle modifications (diet and exercise), oral medications such as \n",
    "    Metformin, and insulin therapy for severe cases.\"\"\",\n",
    "    \n",
    "    retrieval_context=[\n",
    "        \"Type 2 Diabetes symptoms include increased thirst (polydipsia), frequent urination (polyuria), and fatigue.\",\n",
    "        \"Patients may experience blurred vision and slow-healing wounds.\",\n",
    "        \"Treatment involves lifestyle changes including a healthy diet and regular exercise.\",\n",
    "        \"Metformin is a common oral medication prescribed for Type 2 Diabetes management.\",\n",
    "        \"In advanced cases, insulin therapy may be required to control blood sugar levels.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the metric\n",
    "metric = ContextualRecallMetric(\n",
    "    threshold=0.8, \n",
    "    model=\"gpt-4o\", \n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "# Measure contextual recall\n",
    "try:\n",
    "    metric.measure(test_case_medical)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MEDICAL QUERY EVALUATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Contextual Recall Score: {metric.score:.2f}\")\n",
    "    print(f\"\\nReason:\\n{metric.reason}\")\n",
    "    print(f\"\\nPassed Threshold (0.8): {metric.is_successful()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error measuring contextual recall: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16e5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
