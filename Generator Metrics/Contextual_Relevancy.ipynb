{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139e9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c37976",
   "metadata": {},
   "source": [
    "## Example 1: Basic Contextual Relevancy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67fc66eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e33bfd4b3984fb983c5ba2167ec35ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Relevancy Score: 0.167\n",
      "\n",
      "Reason: The score is 0.17 because while the statement 'Paris is the capital and largest city of France.' is directly relevant, the majority of the context provided, such as 'France is a country in Western Europe with a rich history spanning thousands of years.' and 'The Eiffel Tower, located in Paris, is one of the most recognizable landmarks in the world.' do not directly address the capital of France.\n",
      "\n",
      "Success (>= threshold): False\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Define the test case\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What is the capital of France?\",\n",
    "    \n",
    "    # The actual output from your RAG system\n",
    "    actual_output=\"The capital of France is Paris.\",\n",
    "    \n",
    "    # Ground truth - what the complete answer should be\n",
    "    expected_output=\"The capital of France is Paris.\",\n",
    "    \n",
    "    # Retrieved context from your retrieval system\n",
    "    retrieval_context=[\n",
    "        \"\"\"France is a country in Western Europe with a rich history spanning \n",
    "        thousands of years. It is known for its exquisite wine, world-class cheese, \n",
    "        and profound cultural influence. The country has many beautiful cities and \n",
    "        diverse regions. Paris is the capital and largest city of France. The Eiffel \n",
    "        Tower, located in Paris, is one of the most recognizable landmarks in the \n",
    "        world. French cuisine is considered among the finest globally.\"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the metric\n",
    "contextual_relevancy_metric = ContextualRelevancyMetric(\n",
    "    threshold=0.7,  # Minimum acceptable score\n",
    "    model=\"gpt-4o\",  # LLM model to use for evaluation\n",
    "    include_reason=True  # Include reasoning in the output\n",
    ")\n",
    "\n",
    "# Measure the metric\n",
    "contextual_relevancy_metric.measure(test_case)\n",
    "\n",
    "# Print results\n",
    "print(f\"Contextual Relevancy Score: {contextual_relevancy_metric.score:.3f}\")\n",
    "print(f\"\\nReason: {contextual_relevancy_metric.reason}\")\n",
    "print(f\"\\nSuccess (>= threshold): {contextual_relevancy_metric.is_successful()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c9263",
   "metadata": {},
   "source": [
    "## Example 2: Excellent Contextual Relevancy (High Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ede6d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e7832d5925431488231322d551a181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Relevancy Score: 1.000\n",
      "\n",
      "Reason: The score is 1.00 because the statement 'Paris is the capital of France.' directly answers the input question with perfect accuracy. Great job!\n",
      "\n",
      "Success (>= threshold): True\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Test case with concise, focused context\n",
    "test_case_excellent = LLMTestCase(\n",
    "    input=\"What is the capital of France?\",\n",
    "    \n",
    "    actual_output=\"The capital of France is Paris.\",\n",
    "    \n",
    "    expected_output=\"The capital of France is Paris.\",\n",
    "    \n",
    "    # Concise context - all content directly relevant\n",
    "    retrieval_context=[\n",
    "        \"Paris is the capital of France.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize and measure\n",
    "contextual_relevancy_metric = ContextualRelevancyMetric(\n",
    "    threshold=0.9,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextual_relevancy_metric.measure(test_case_excellent)\n",
    "\n",
    "# Print results\n",
    "print(f\"Contextual Relevancy Score: {contextual_relevancy_metric.score:.3f}\")\n",
    "print(f\"\\nReason: {contextual_relevancy_metric.reason}\")\n",
    "print(f\"\\nSuccess (>= threshold): {contextual_relevancy_metric.is_successful()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948609d",
   "metadata": {},
   "source": [
    "## Example 3: Moderate Contextual Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d51db536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be6937e71f24a43b5291a7c5c03a8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Relevancy Score: 0.667\n",
      "\n",
      "Reason: The score is 0.67 because while the context includes relevant statements like 'Python is a high-level, interpreted programming language' and 'Python emphasizes code readability with significant indentation,' it also contains irrelevant historical details such as 'It was created by Guido van Rossum and first released in 1991,' which do not pertain to the key features of Python.\n",
      "\n",
      "Success (>= threshold): True\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Test case with some relevant and some background content\n",
    "test_case_moderate = LLMTestCase(\n",
    "    input=\"What are the key features of Python programming language?\",\n",
    "    \n",
    "    actual_output=\"\"\"Python is a high-level, interpreted programming language \n",
    "    with simple syntax, supports multiple programming paradigms, and has \n",
    "    extensive libraries.\"\"\",\n",
    "    \n",
    "    expected_output=\"\"\"Python is a high-level, interpreted language with simple \n",
    "    and readable syntax, supports multiple programming paradigms, and has \n",
    "    extensive libraries for various applications.\"\"\",\n",
    "    \n",
    "    # Mixed context - some relevant features, some history\n",
    "    retrieval_context=[\n",
    "        \"\"\"Python is a high-level, interpreted programming language. It was \n",
    "        created by Guido van Rossum and first released in 1991. The language \n",
    "        was named after the British comedy group Monty Python. Python emphasizes \n",
    "        code readability with significant indentation. It supports multiple \n",
    "        programming paradigms including object-oriented, procedural, and functional \n",
    "        programming. Python has a comprehensive standard library and extensive \n",
    "        third-party packages available through PyPI.\"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Measure\n",
    "metric = ContextualRelevancyMetric(\n",
    "    threshold=0.6,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "metric.measure(test_case_moderate)\n",
    "\n",
    "print(f\"Contextual Relevancy Score: {metric.score:.3f}\")\n",
    "print(f\"\\nReason: {metric.reason}\")\n",
    "print(f\"\\nSuccess (>= threshold): {metric.is_successful()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412ab9f",
   "metadata": {},
   "source": [
    "## Example 4: Medical Query with Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba4d75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3f52e05e054934bc8b765dc32e8293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MEDICAL QUERY EVALUATION\n",
      "======================================================================\n",
      "Contextual Relevancy Score: 0.125\n",
      "\n",
      "Reason:\n",
      "The score is 0.12 because the majority of the context focuses on aspects unrelated to symptoms, such as causes, prevalence, and treatment options. Only one statement directly addresses the symptoms of Type 2 Diabetes, mentioning 'increased thirst (polydipsia), frequent urination (polyuria), increased hunger, fatigue, blurred vision, and slow-healing wounds or frequent infections.'\n",
      "\n",
      "Passed Threshold (0.5): False\n",
      "\n",
      "======================================================================\n",
      "SENTENCE-LEVEL RELEVANCE ANALYSIS\n",
      "======================================================================\n",
      "Sentence 1: 'Type 2 Diabetes is a chronic metabolic disorder...' - BACKGROUND ‚úó\n",
      "Sentence 2: 'The condition was first distinguished from Type 1...' - HISTORY ‚úó\n",
      "Sentence 3: 'It is the most common form of diabetes...' - STATISTICS ‚úó\n",
      "Sentence 4: 'The disease develops when the body becomes resistant...' - MECHANISM ‚úó\n",
      "Sentence 5: 'Common symptoms include increased thirst...' - SYMPTOMS ‚úì‚úì‚úì\n",
      "Sentence 6: 'Risk factors include obesity, sedentary lifestyle...' - RISK FACTORS ‚úó\n",
      "Sentence 7: 'The pancreas plays a key role...' - ANATOMY ‚úó\n",
      "Sentence 8: 'Treatment options include lifestyle modifications...' - TREATMENT ‚úó\n",
      "\n",
      "Relevant: 1 sentence out of 8 (~12.5%)\n",
      "However, that 1 sentence contains ALL the symptom information needed\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Medical information retrieval scenario\n",
    "test_case_medical = LLMTestCase(\n",
    "    input=\"What are the symptoms of Type 2 Diabetes?\",\n",
    "    \n",
    "    actual_output=\"\"\"Type 2 Diabetes symptoms include increased thirst, frequent \n",
    "    urination, fatigue, blurred vision, and slow-healing wounds.\"\"\",\n",
    "    \n",
    "    expected_output=\"\"\"Type 2 Diabetes presents with symptoms including increased \n",
    "    thirst, frequent urination, unexplained weight loss, fatigue, blurred vision, \n",
    "    and slow-healing wounds.\"\"\",\n",
    "    \n",
    "    retrieval_context=[\n",
    "        \"\"\"Type 2 Diabetes is a chronic metabolic disorder that affects the way \n",
    "        the body processes blood sugar (glucose). The condition was first \n",
    "        distinguished from Type 1 Diabetes in 1936. It is the most common form \n",
    "        of diabetes, affecting millions of people worldwide. The disease develops \n",
    "        when the body becomes resistant to insulin or doesn't produce enough insulin. \n",
    "        Common symptoms include increased thirst (polydipsia), frequent urination \n",
    "        (polyuria), increased hunger, fatigue, blurred vision, and slow-healing \n",
    "        wounds or frequent infections. Risk factors include obesity, sedentary \n",
    "        lifestyle, family history, and age over 45. The pancreas plays a key role \n",
    "        in insulin production. Treatment options include lifestyle modifications, \n",
    "        oral medications, and insulin therapy in severe cases.\"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Measure contextual relevancy\n",
    "metric = ContextualRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "metric.measure(test_case_medical)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MEDICAL QUERY EVALUATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Contextual Relevancy Score: {metric.score:.3f}\")\n",
    "print(f\"\\nReason:\\n{metric.reason}\")\n",
    "print(f\"\\nPassed Threshold (0.5): {metric.is_successful()}\")\n",
    "\n",
    "# Manual breakdown\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SENTENCE-LEVEL RELEVANCE ANALYSIS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"Sentence 1: 'Type 2 Diabetes is a chronic metabolic disorder...' - BACKGROUND ‚úó\")\n",
    "print(\"Sentence 2: 'The condition was first distinguished from Type 1...' - HISTORY ‚úó\")\n",
    "print(\"Sentence 3: 'It is the most common form of diabetes...' - STATISTICS ‚úó\")\n",
    "print(\"Sentence 4: 'The disease develops when the body becomes resistant...' - MECHANISM ‚úó\")\n",
    "print(\"Sentence 5: 'Common symptoms include increased thirst...' - SYMPTOMS ‚úì‚úì‚úì\")\n",
    "print(\"Sentence 6: 'Risk factors include obesity, sedentary lifestyle...' - RISK FACTORS ‚úó\")\n",
    "print(\"Sentence 7: 'The pancreas plays a key role...' - ANATOMY ‚úó\")\n",
    "print(\"Sentence 8: 'Treatment options include lifestyle modifications...' - TREATMENT ‚úó\")\n",
    "print(\"\\nRelevant: 1 sentence out of 8 (~12.5%)\")\n",
    "print(\"However, that 1 sentence contains ALL the symptom information needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3230fb",
   "metadata": {},
   "source": [
    "## Example 5: Batch Evaluation with Multiple Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "881f75c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c11488df29f4cfa9df072240826c63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BATCH EVALUATION RESULTS\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893f22dbcaf04b3ba7a01a0b497ea81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case 1:\n",
      "  Query: What is the capital of Japan?\n",
      "  Contextual Relevancy: 1.000\n",
      "  Passed (>=0.7): True\n",
      "  Reason: The score is 1.00 because the statement 'Tokyo is the capital and largest city of Japan.' directly answers the input question with perfect relevance. ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b31c37a2b049f99da3aeb356623d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case 2:\n",
      "  Query: Who invented the telephone?\n",
      "  Contextual Relevancy: 0.125\n",
      "  Passed (>=0.7): False\n",
      "  Reason: The score is 0.12 because the retrieval context primarily contains general information about communication technology and its evolution, which does no...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case 3:\n",
      "  Query: What are the primary colors?\n",
      "  Contextual Relevancy: 1.000\n",
      "  Passed (>=0.7): True\n",
      "  Reason: The score is 1.00 because the retrieval context perfectly matches the input, providing clear and accurate information about the primary colors. Great ...\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Create multiple test cases with varying relevancy levels\n",
    "test_cases = [\n",
    "    # Case 1: Excellent relevancy (concise)\n",
    "    LLMTestCase(\n",
    "        input=\"What is the capital of Japan?\",\n",
    "        expected_output=\"The capital of Japan is Tokyo.\",\n",
    "        retrieval_context=[\n",
    "            \"Tokyo is the capital and largest city of Japan.\"\n",
    "        ]\n",
    "    ),\n",
    "    \n",
    "    # Case 2: Poor relevancy (verbose)\n",
    "    LLMTestCase(\n",
    "        input=\"Who invented the telephone?\",\n",
    "        expected_output=\"Alexander Graham Bell invented the telephone in 1876.\",\n",
    "        retrieval_context=[\n",
    "            \"\"\"The history of communication technology is fascinating. From ancient \n",
    "            smoke signals to modern smartphones, humans have always sought better \n",
    "            ways to communicate. The telegraph revolutionized long-distance \n",
    "            communication in the 1830s. Alexander Graham Bell, a Scottish-born \n",
    "            inventor, is credited with inventing the telephone in 1876. Bell's \n",
    "            interest in sound and speech stemmed from his work with the deaf. His \n",
    "            wife was deaf, which influenced his research. The telephone transformed \n",
    "            society and business. Today, billions of people use mobile phones daily.\"\"\"\n",
    "        ]\n",
    "    ),\n",
    "    \n",
    "    # Case 3: Good relevancy (mostly focused)\n",
    "    LLMTestCase(\n",
    "        input=\"What are the primary colors?\",\n",
    "        expected_output=\"The primary colors are red, blue, and yellow.\",\n",
    "        retrieval_context=[\n",
    "            \"\"\"The primary colors are red, blue, and yellow. These three colors \n",
    "            cannot be created by mixing other colors together. All other colors \n",
    "            can be created by mixing primary colors in different combinations.\"\"\"\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define metric\n",
    "contextual_relevancy = ContextualRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "# Evaluate each test case\n",
    "print(\"=\"*70)\n",
    "print(\"BATCH EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    # Measure metric\n",
    "    contextual_relevancy.measure(test_case)\n",
    "    \n",
    "    print(f\"\\nTest Case {i}:\")\n",
    "    print(f\"  Query: {test_case.input}\")\n",
    "    print(f\"  Contextual Relevancy: {contextual_relevancy.score:.3f}\")\n",
    "    print(f\"  Passed (>=0.7): {contextual_relevancy.is_successful()}\")\n",
    "    print(f\"  Reason: {contextual_relevancy.reason[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc6a1e9",
   "metadata": {},
   "source": [
    "## Example 6: Comparing Concise vs Verbose Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97be6708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328bde4fa43c4ff48cd0ad6b746e6820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87b241aa5904c839c63f5d50653ef19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERBOSITY IMPACT COMPARISON\n",
      "======================================================================\n",
      "\n",
      "üìä SCENARIO A: Concise Context\n",
      "   Total sentences: ~2\n",
      "   Relevant sentences: ~2\n",
      "   Score: 1.000\n",
      "   Assessment: The score is 1.00 because the retrieval context perfectly aligns with the input, providing a clear a...\n",
      "\n",
      "üìä SCENARIO B: Verbose Context\n",
      "   Total sentences: ~10\n",
      "   Relevant sentences: ~2\n",
      "   Score: 0.111\n",
      "   Assessment: The score is 0.11 because only one statement, 'Machine learning is a subset of artificial intelligen...\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS\n",
      "======================================================================\n",
      "Score Difference: 0.889\n",
      "Verbosity Penalty: 88.9%\n",
      "\n",
      "üí° Key Insight: Same relevant information, but verbose context has\n",
      "   ~80% noise (AI history, companies, ethics) that dilutes relevancy\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Same query, different context verbosity\n",
    "\n",
    "# Scenario A: Concise Context (High Relevancy)\n",
    "test_case_concise = LLMTestCase(\n",
    "    input=\"What is machine learning?\",\n",
    "    expected_output=\"Machine learning is a subset of AI where systems learn from data without explicit programming.\",\n",
    "    retrieval_context=[\n",
    "        \"\"\"Machine learning is a subset of artificial intelligence where computer \n",
    "        systems learn and improve from experience without being explicitly programmed. \n",
    "        ML algorithms use statistical techniques to identify patterns in data.\"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scenario B: Verbose Context (Low Relevancy)\n",
    "test_case_verbose = LLMTestCase(\n",
    "    input=\"What is machine learning?\",\n",
    "    expected_output=\"Machine learning is a subset of AI where systems learn from data without explicit programming.\",\n",
    "    retrieval_context=[\n",
    "        \"\"\"Artificial intelligence has been a subject of fascination since the 1950s. \n",
    "        Early pioneers like Alan Turing laid the groundwork for modern computing. \n",
    "        The field has seen many ups and downs, including periods called \"AI winters\" \n",
    "        when funding dried up. In the 1980s, expert systems were popular but had \n",
    "        limitations. Machine learning is a subset of artificial intelligence where \n",
    "        computer systems learn from experience without explicit programming. Neural \n",
    "        networks, inspired by biological neurons, have become increasingly important. \n",
    "        Deep learning, a subset of machine learning, uses multi-layered neural \n",
    "        networks. Companies like Google, Amazon, and Facebook invest heavily in AI \n",
    "        research. The future of AI raises ethical questions about automation and \n",
    "        employment.\"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate both\n",
    "metric_concise = ContextualRelevancyMetric(threshold=0.7, model=\"gpt-4o\", include_reason=True)\n",
    "metric_verbose = ContextualRelevancyMetric(threshold=0.7, model=\"gpt-4o\", include_reason=True)\n",
    "\n",
    "metric_concise.measure(test_case_concise)\n",
    "metric_verbose.measure(test_case_verbose)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERBOSITY IMPACT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä SCENARIO A: Concise Context\")\n",
    "print(f\"   Total sentences: ~2\")\n",
    "print(f\"   Relevant sentences: ~2\")\n",
    "print(f\"   Score: {metric_concise.score:.3f}\")\n",
    "print(f\"   Assessment: {metric_concise.reason[:100]}...\")\n",
    "\n",
    "print(\"\\nüìä SCENARIO B: Verbose Context\")\n",
    "print(f\"   Total sentences: ~10\")\n",
    "print(f\"   Relevant sentences: ~2\")\n",
    "print(f\"   Score: {metric_verbose.score:.3f}\")\n",
    "print(f\"   Assessment: {metric_verbose.reason[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Score Difference: {abs(metric_concise.score - metric_verbose.score):.3f}\")\n",
    "print(f\"Verbosity Penalty: {((metric_concise.score - metric_verbose.score) / metric_concise.score * 100):.1f}%\")\n",
    "print(\"\\nüí° Key Insight: Same relevant information, but verbose context has\")\n",
    "print(\"   ~80% noise (AI history, companies, ethics) that dilutes relevancy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b64154f",
   "metadata": {},
   "source": [
    "## Example 7: Real-World RAG Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa4809b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868fbedf382c406daa1eb260de8527e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RAG PIPELINE - CONTEXTUAL RELEVANCY EVALUATION\n",
      "======================================================================\n",
      "\n",
      "üìù Query: What are the key features of Databricks?\n",
      "\n",
      "üìö Retrieved 3 chunks\n",
      "\n",
      "Chunk 1 Preview:\n",
      "  Databricks was founded in 2013 by the creators of Apache Spark. The company \n",
      "    is headquartered in...\n",
      "\n",
      "Chunk 2 Preview:\n",
      "  Databricks provides a cloud-based platform for data engineering and data science \n",
      "    teams. Key fea...\n",
      "\n",
      "Chunk 3 Preview:\n",
      "  Databricks features include MLflow for machine learning lifecycle management, \n",
      "    AutoML for automa...\n",
      "\n",
      "üìä Contextual Relevancy Score: 0.500\n",
      "\n",
      "üí° Evaluation:\n",
      "The score is 0.50 because while the retrieval context includes relevant statements like 'Databricks is a unified analytics platform built on Apache Spark for big data processing and machine learning' and 'Key features include collaborative notebooks for writing code, automated cluster management for scaling compute resources, and Delta Lake for reliable data lakes with ACID transactions,' it also contains several irrelevant details about the company's origin, location, financial status, and client base.\n",
      "\n",
      "‚úÖ Passed Threshold (0.6): False\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDATIONS\n",
      "======================================================================\n",
      "‚ö†Ô∏è  Relevancy below optimal\n",
      "   - Consider increasing similarity threshold to filter verbose chunks\n",
      "   - Implement sentence-level filtering to extract only relevant content\n",
      "   - Review chunking strategy - create more focused, topic-specific chunks\n",
      "   - Add contextual compression layer to remove background information\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from typing import List\n",
    "\n",
    "# Simulate a RAG pipeline\n",
    "class RAGPipeline:\n",
    "    def __init__(self, knowledge_base: List[str], chunk_size: int = 500):\n",
    "        self.knowledge_base = knowledge_base\n",
    "        self.chunk_size = chunk_size\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"Simulated retrieval\"\"\"\n",
    "        # In reality, this would use vector similarity search\n",
    "        # For demo, we'll return pre-selected chunks\n",
    "        return self.knowledge_base[:top_k]\n",
    "    \n",
    "    def generate(self, query: str, context: List[str]) -> str:\n",
    "        \"\"\"Simulated generation\"\"\"\n",
    "        return f\"Answer based on context: {context[0][:50]}...\"\n",
    "\n",
    "# Knowledge base with varying verbosity\n",
    "knowledge_base = [\n",
    "    # Chunk 1: Verbose - lots of background\n",
    "    \"\"\"Databricks was founded in 2013 by the creators of Apache Spark. The company \n",
    "    is headquartered in San Francisco, California. It has raised billions in funding \n",
    "    and is valued as a decacorn. Databricks is a unified analytics platform built \n",
    "    on Apache Spark for big data processing and machine learning. The platform serves \n",
    "    thousands of enterprise customers worldwide. Notable clients include Shell, \n",
    "    Comcast, and H&M. The company has expanded globally with offices in multiple countries.\"\"\",\n",
    "    \n",
    "    # Chunk 2: Moderate - some background, some features\n",
    "    \"\"\"Databricks provides a cloud-based platform for data engineering and data science \n",
    "    teams. Key features include collaborative notebooks for writing code, automated \n",
    "    cluster management for scaling compute resources, and Delta Lake for reliable \n",
    "    data lakes with ACID transactions.\"\"\",\n",
    "    \n",
    "    # Chunk 3: Concise - focused on features\n",
    "    \"\"\"Databricks features include MLflow for machine learning lifecycle management, \n",
    "    AutoML for automated model training, and Unity Catalog for unified data governance.\"\"\"\n",
    "]\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = RAGPipeline(knowledge_base)\n",
    "\n",
    "# Test query\n",
    "query = \"What are the key features of Databricks?\"\n",
    "expected = \"\"\"Databricks features include collaborative notebooks, automated cluster \n",
    "management, Delta Lake, MLflow, AutoML, and Unity Catalog.\"\"\"\n",
    "\n",
    "# Retrieve\n",
    "retrieved = pipeline.retrieve(query, top_k=3)\n",
    "\n",
    "# Evaluate relevancy\n",
    "test_case = LLMTestCase(\n",
    "    input=query,\n",
    "    expected_output=expected,\n",
    "    retrieval_context=retrieved\n",
    ")\n",
    "\n",
    "metric = ContextualRelevancyMetric(threshold=0.6, model=\"gpt-4o\", include_reason=True)\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RAG PIPELINE - CONTEXTUAL RELEVANCY EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìù Query: {query}\")\n",
    "print(f\"\\nüìö Retrieved {len(retrieved)} chunks\")\n",
    "\n",
    "for i, chunk in enumerate(retrieved, 1):\n",
    "    print(f\"\\nChunk {i} Preview:\")\n",
    "    print(f\"  {chunk[:100]}...\")\n",
    "\n",
    "print(f\"\\nüìä Contextual Relevancy Score: {metric.score:.3f}\")\n",
    "print(f\"\\nüí° Evaluation:\\n{metric.reason}\")\n",
    "print(f\"\\n‚úÖ Passed Threshold (0.6): {metric.is_successful()}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(f\"{'='*70}\")\n",
    "if metric.score < 0.7:\n",
    "    print(\"‚ö†Ô∏è  Relevancy below optimal\")\n",
    "    print(\"   - Consider increasing similarity threshold to filter verbose chunks\")\n",
    "    print(\"   - Implement sentence-level filtering to extract only relevant content\")\n",
    "    print(\"   - Review chunking strategy - create more focused, topic-specific chunks\")\n",
    "    print(\"   - Add contextual compression layer to remove background information\")\n",
    "else:\n",
    "    print(\"‚úÖ Good relevancy\")\n",
    "    print(\"   - Monitor consistency across different query types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2cadbe",
   "metadata": {},
   "source": [
    "## Example 8: Understanding LLM-Based Relevancy Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06b921eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692c881e9b2849e88a1847c3e12983a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LLM-BASED RELEVANCY ASSESSMENT DEMONSTRATION\n",
      "======================================================================\n",
      "\n",
      "üîç How DeepEval Evaluates Sentence Relevancy:\n",
      "\n",
      "The LLM analyzes each sentence and asks:\n",
      "  'Does this sentence directly help answer: What is photosynthesis??'\n",
      "\n",
      "Sentence 1: 'Plants are living organisms that belong to the kingdom Plantae.'\n",
      "  LLM Assessment: IRRELEVANT ‚úó\n",
      "  Reasoning: General plant information, doesn't explain photosynthesis\n",
      "\n",
      "Sentence 2: 'They have been on Earth for hundreds of millions of years.'\n",
      "  LLM Assessment: IRRELEVANT ‚úó\n",
      "  Reasoning: Historical information, not about the process\n",
      "\n",
      "Sentence 3: 'Plants play a crucial role in ecosystems.'\n",
      "  LLM Assessment: IRRELEVANT ‚úó\n",
      "  Reasoning: Ecological role, not the process itself\n",
      "\n",
      "Sentence 4: 'Photosynthesis is the process by which plants convert...'\n",
      "  LLM Assessment: RELEVANT ‚úì‚úì‚úì\n",
      "  Reasoning: Directly defines photosynthesis with inputs and outputs\n",
      "\n",
      "Sentence 5: 'This process occurs in chloroplasts, which contain chlorophyll.'\n",
      "  LLM Assessment: PARTIALLY RELEVANT ~\n",
      "  Reasoning: Provides location detail but not core process explanation\n",
      "\n",
      "Sentence 6: 'Most plants are green due to chlorophyll.'\n",
      "  LLM Assessment: IRRELEVANT ‚úó\n",
      "  Reasoning: About color, not the process\n",
      "\n",
      "Sentence 7: 'Plants provide food and oxygen for most life on Earth.'\n",
      "  LLM Assessment: IRRELEVANT ‚úó\n",
      "  Reasoning: About impact, not the process\n",
      "\n",
      "======================================================================\n",
      "üìä Final Contextual Relevancy Score: 0.286\n",
      "======================================================================\n",
      "\n",
      "Calculation:\n",
      "  Relevant sentences: ~1.5 out of 7\n",
      "  Relevancy: 1.5 / 7 = 0.214 (~21%)\n",
      "\n",
      "The score is 0.29 because while the context includes a relevant statement explaining that 'Photosynthesis is the process by which plants convert light energy into chemical energy,' most of the context focuses on general information about plants, such as 'Plants are living organisms that belong to the kingdom Plantae,' which does not directly address the process of photosynthesis.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Demonstrate how DeepEval uses LLM to assess relevancy\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What is photosynthesis?\",\n",
    "    \n",
    "    expected_output=\"\"\"Photosynthesis is the process where plants use sunlight, \n",
    "    water, and carbon dioxide to produce glucose and oxygen.\"\"\",\n",
    "    \n",
    "    retrieval_context=[\n",
    "        \"\"\"Plants are living organisms that belong to the kingdom Plantae. They \n",
    "        have been on Earth for hundreds of millions of years. Plants play a crucial \n",
    "        role in ecosystems. Photosynthesis is the process by which plants convert \n",
    "        light energy into chemical energy, using sunlight, water, and carbon dioxide \n",
    "        to produce glucose and release oxygen. This process occurs in chloroplasts, \n",
    "        which contain chlorophyll. Most plants are green due to chlorophyll. Plants \n",
    "        provide food and oxygen for most life on Earth.\"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create metric\n",
    "metric = ContextualRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LLM-BASED RELEVANCY ASSESSMENT DEMONSTRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüîç How DeepEval Evaluates Sentence Relevancy:\\n\")\n",
    "print(\"The LLM analyzes each sentence and asks:\")\n",
    "print(f\"  'Does this sentence directly help answer: {test_case.input}?'\\n\")\n",
    "\n",
    "print(\"Sentence 1: 'Plants are living organisms that belong to the kingdom Plantae.'\")\n",
    "print(\"  LLM Assessment: IRRELEVANT ‚úó\")\n",
    "print(\"  Reasoning: General plant information, doesn't explain photosynthesis\\n\")\n",
    "\n",
    "print(\"Sentence 2: 'They have been on Earth for hundreds of millions of years.'\")\n",
    "print(\"  LLM Assessment: IRRELEVANT ‚úó\")\n",
    "print(\"  Reasoning: Historical information, not about the process\\n\")\n",
    "\n",
    "print(\"Sentence 3: 'Plants play a crucial role in ecosystems.'\")\n",
    "print(\"  LLM Assessment: IRRELEVANT ‚úó\")\n",
    "print(\"  Reasoning: Ecological role, not the process itself\\n\")\n",
    "\n",
    "print(\"Sentence 4: 'Photosynthesis is the process by which plants convert...'\")\n",
    "print(\"  LLM Assessment: RELEVANT ‚úì‚úì‚úì\")\n",
    "print(\"  Reasoning: Directly defines photosynthesis with inputs and outputs\\n\")\n",
    "\n",
    "print(\"Sentence 5: 'This process occurs in chloroplasts, which contain chlorophyll.'\")\n",
    "print(\"  LLM Assessment: PARTIALLY RELEVANT ~\")\n",
    "print(\"  Reasoning: Provides location detail but not core process explanation\\n\")\n",
    "\n",
    "print(\"Sentence 6: 'Most plants are green due to chlorophyll.'\")\n",
    "print(\"  LLM Assessment: IRRELEVANT ‚úó\")\n",
    "print(\"  Reasoning: About color, not the process\\n\")\n",
    "\n",
    "print(\"Sentence 7: 'Plants provide food and oxygen for most life on Earth.'\")\n",
    "print(\"  LLM Assessment: IRRELEVANT ‚úó\")\n",
    "print(\"  Reasoning: About impact, not the process\\n\")\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"üìä Final Contextual Relevancy Score: {metric.score:.3f}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nCalculation:\")\n",
    "print(f\"  Relevant sentences: ~1.5 out of 7\")\n",
    "print(f\"  Relevancy: 1.5 / 7 = 0.214 (~21%)\")\n",
    "print(f\"\\n{metric.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70361a9e",
   "metadata": {},
   "source": [
    "## Complete Working Example with Detailed Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "def demonstrate_contextual_relevancy():\n",
    "    \"\"\"Complete demonstration of relevancy evaluation\"\"\"\n",
    "    \n",
    "    # Scenario 1: Perfect Relevancy (Concise)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 1: PERFECT RELEVANCY (Concise Context)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_perfect = LLMTestCase(\n",
    "        input=\"What is the boiling point of water?\",\n",
    "        expected_output=\"Water boils at 100¬∞C (212¬∞F) at sea level.\",\n",
    "        retrieval_context=[\n",
    "            \"Water boils at 100 degrees Celsius at standard atmospheric pressure, which is 212 degrees Fahrenheit.\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    metric_perfect = ContextualRelevancyMetric(threshold=0.9, model=\"gpt-4\", include_reason=True)\n",
    "    metric_perfect.measure(test_perfect)\n",
    "    \n",
    "    print(f\"Query: {test_perfect.input}\")\n",
    "    print(f\"\\nContext Length: ~15 words\")\n",
    "    print(f\"Relevant Content: ~15 words (100%)\")\n",
    "    print(f\"üìä Relevancy Score: {metric_perfect.score:.3f}\")\n",
    "    print(f\"‚úÖ Perfectly focused - no waste\")\n",
    "    \n",
    "    # Scenario 2: Poor Relevancy (Verbose)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 2: POOR RELEVANCY (Verbose Context)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_poor = LLMTestCase(\n",
    "        input=\"What is the boiling point of water?\",\n",
    "        expected_output=\"Water boils at 100¬∞C (212¬∞F) at sea level.\",\n",
    "        retrieval_context=[\n",
    "            \"\"\"Water is a chemical compound consisting of two hydrogen atoms and \n",
    "            one oxygen atom. It covers about 71% of Earth's surface. Water is \n",
    "            essential for all known forms of life. The ancient Greeks considered \n",
    "            water one of the four classical elements. Water has been central to \n",
    "            human civilization throughout history. At standard atmospheric pressure, \n",
    "            water boils at 100 degrees Celsius or 212 degrees Fahrenheit. The Dead \n",
    "            Sea has the highest salinity of any body of water. Water has many unique \n",
    "            properties including high surface tension.\"\"\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    metric_poor = ContextualRelevancyMetric(threshold=0.5, model=\"gpt-4\", include_reason=True)\n",
    "    metric_poor.measure(test_poor)\n",
    "    \n",
    "    print(f\"Query: {test_poor.input}\")\n",
    "    print(f\"\\nContext Length: ~95 words\")\n",
    "    print(f\"Relevant Content: ~15 words (16%)\")\n",
    "    print(f\"Noise: ~80 words (84%)\")\n",
    "    print(f\"üìä Relevancy Score: {metric_poor.score:.3f}\")\n",
    "    print(f\"‚ö†Ô∏è  Excessive background information\")\n",
    "    \n",
    "    # Scenario 3: Moderate Relevancy\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 3: MODERATE RELEVANCY (Mixed Content)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_moderate = LLMTestCase(\n",
    "        input=\"What is the boiling point of water?\",\n",
    "        expected_output=\"Water boils at 100¬∞C (212¬∞F) at sea level.\",\n",
    "        retrieval_context=[\n",
    "            \"\"\"Water boils when it reaches its boiling point temperature. At standard \n",
    "            atmospheric pressure (sea level), water boils at 100 degrees Celsius, \n",
    "            which equals 212 degrees Fahrenheit. At higher altitudes, where air \n",
    "            pressure is lower, water boils at lower temperatures. This is why cooking \n",
    "            times are different in mountain regions.\"\"\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    metric_moderate = ContextualRelevancyMetric(threshold=0.6, model=\"gpt-4\", include_reason=True)\n",
    "    metric_moderate.measure(test_moderate)\n",
    "    \n",
    "    print(f\"Query: {test_moderate.input}\")\n",
    "    print(f\"\\nContext Length: ~60 words\")\n",
    "    print(f\"Relevant Content: ~20 words (33%)\")\n",
    "    print(f\"Additional Context: ~40 words (67%)\")\n",
    "    print(f\"üìä Relevancy Score: {metric_moderate.score:.3f}\")\n",
    "    print(f\"‚ö†Ô∏è  Answer present but with extra altitude information\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Scenario':<30} {'Context Size':<15} {'Relevancy':<12} {'Status'}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Perfect (Concise)':<30} {'15 words':<15} {metric_perfect.score:.3f}       {'Excellent ‚úÖ'}\")\n",
    "    print(f\"{'Moderate (Mixed)':<30} {'60 words':<15} {metric_moderate.score:.3f}       {'Acceptable ‚ö†Ô∏è'}\")\n",
    "    print(f\"{'Poor (Verbose)':<30} {'95 words':<15} {metric_poor.score:.3f}       {'Poor ‚ùå'}\")\n",
    "    \n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    print(\"   ‚Ä¢ Concise: 100% relevant ‚Üí Perfect score\")\n",
    "    print(\"   ‚Ä¢ Mixed: Some relevant, some context ‚Üí Moderate score\")\n",
    "    print(\"   ‚Ä¢ Verbose: Buried in noise ‚Üí Poor score\")\n",
    "    print(\"   ‚Ä¢ Same answer quality, different efficiency\")\n",
    "\n",
    "# Run demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_contextual_relevancy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7357b2c",
   "metadata": {},
   "source": [
    "## Code Example: Demonstrating Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73130b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMONSTRATING CONTEXTUAL RELEVANCY DEPENDENCIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Base scenario\n",
    "base_query = \"What is the capital of France?\"\n",
    "base_expected = \"The capital of France is Paris.\"\n",
    "base_context = [\n",
    "    \"France is a country in Western Europe known for wine and cheese. \"\n",
    "    \"Paris is the capital and largest city. The Eiffel Tower is a famous landmark.\"\n",
    "]\n",
    "\n",
    "# Test 1: Change RETRIEVED CONTEXT (Primary Dependency)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 1: CHANGING RETRIEVED CONTEXT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Concise context (high density)\n",
    "context_concise = [\n",
    "    \"Paris is the capital of France.\"\n",
    "]\n",
    "\n",
    "# Verbose context (low density)\n",
    "context_verbose = [\n",
    "    \"France is a country in Western Europe with a rich cultural heritage. \"\n",
    "    \"It is known for wine, cheese, fashion, and art. The country has many \"\n",
    "    \"beautiful cities and regions. Paris, which has many museums and restaurants, \"\n",
    "    \"is the capital city. The Eiffel Tower is located there. French cuisine \"\n",
    "    \"is considered among the finest in the world.\"\n",
    "]\n",
    "\n",
    "test_concise = LLMTestCase(\n",
    "    input=base_query,\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=context_concise\n",
    ")\n",
    "\n",
    "test_verbose = LLMTestCase(\n",
    "    input=base_query,\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=context_verbose\n",
    ")\n",
    "\n",
    "metric = ContextualRelevancyMetric(threshold=0.5, model=\"gpt-4\")\n",
    "\n",
    "metric.measure(test_concise)\n",
    "score_concise = metric.score\n",
    "\n",
    "metric.measure(test_verbose)\n",
    "score_verbose = metric.score\n",
    "\n",
    "print(f\"Concise Context (7 words):\")\n",
    "print(f\"  All content relevant to query\")\n",
    "print(f\"  Relevancy Score: {score_concise:.3f}\")\n",
    "\n",
    "print(f\"\\nVerbose Context (65 words):\")\n",
    "print(f\"  Only ~5 words relevant ('Paris is the capital')\")\n",
    "print(f\"  Relevancy Score: {score_verbose:.3f}\")\n",
    "\n",
    "print(f\"\\nImpact: {abs(score_concise - score_verbose):.3f} difference\")\n",
    "print(\"‚úÖ HUGE IMPACT - Retrieved Context is PRIMARY dependency\")\n",
    "\n",
    "# Test 2: Change INPUT QUERY (Primary Dependency)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 2: CHANGING INPUT QUERY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "context_mixed = [\n",
    "    \"Paris is the capital of France. The city has a population of 2.1 million. \"\n",
    "    \"It is known for the Eiffel Tower and the Louvre Museum.\"\n",
    "]\n",
    "\n",
    "# Query 1: Specific (only wants capital)\n",
    "query_specific = \"What is the capital of France?\"\n",
    "\n",
    "# Query 2: Broad (wants general info about Paris)\n",
    "query_broad = \"Tell me about Paris\"\n",
    "\n",
    "test_specific_query = LLMTestCase(\n",
    "    input=query_specific,\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=context_mixed\n",
    ")\n",
    "\n",
    "test_broad_query = LLMTestCase(\n",
    "    input=query_broad,\n",
    "    expected_output=\"Paris is the capital of France with 2.1M population, known for Eiffel Tower and Louvre.\",\n",
    "    retrieval_context=context_mixed\n",
    ")\n",
    "\n",
    "metric.measure(test_specific_query)\n",
    "score_specific = metric.score\n",
    "\n",
    "metric.measure(test_broad_query)\n",
    "score_broad = metric.score\n",
    "\n",
    "print(f\"Specific Query ('What is the capital?'):\")\n",
    "print(f\"  Only 'Paris is the capital' relevant (~7 words)\")\n",
    "print(f\"  Total context: ~30 words\")\n",
    "print(f\"  Relevancy Score: {score_specific:.3f}\")\n",
    "\n",
    "print(f\"\\nBroad Query ('Tell me about Paris'):\")\n",
    "print(f\"  All information about Paris relevant (~30 words)\")\n",
    "print(f\"  Total context: ~30 words\")\n",
    "print(f\"  Relevancy Score: {score_broad:.3f}\")\n",
    "\n",
    "print(f\"\\nImpact: {abs(score_specific - score_broad):.3f} difference\")\n",
    "print(\"‚úÖ HUGE IMPACT - Input Query defines what's 'relevant'\")\n",
    "\n",
    "# Test 3: Change EXPECTED OUTPUT (Secondary Dependency)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 3: CHANGING EXPECTED OUTPUT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "context_detailed = [\n",
    "    \"Python is a high-level programming language created by Guido van Rossum \"\n",
    "    \"in 1991. It emphasizes code readability with significant whitespace. \"\n",
    "    \"Python supports multiple paradigms including OOP and functional programming.\"\n",
    "]\n",
    "\n",
    "# Expected 1: Minimal (just definition)\n",
    "expected_minimal = \"Python is a programming language\"\n",
    "\n",
    "# Expected 2: Detailed (wants multiple facts)\n",
    "expected_detailed = \"Python is a high-level language with readable syntax supporting multiple paradigms\"\n",
    "\n",
    "test_minimal_expected = LLMTestCase(\n",
    "    input=\"What is Python?\",\n",
    "    expected_output=expected_minimal,\n",
    "    retrieval_context=context_detailed\n",
    ")\n",
    "\n",
    "test_detailed_expected = LLMTestCase(\n",
    "    input=\"What is Python?\",\n",
    "    expected_output=expected_detailed,\n",
    "    retrieval_context=context_detailed\n",
    ")\n",
    "\n",
    "metric.measure(test_minimal_expected)\n",
    "score_minimal_exp = metric.score\n",
    "\n",
    "metric.measure(test_detailed_expected)\n",
    "score_detailed_exp = metric.score\n",
    "\n",
    "print(f\"Minimal Expected Output:\")\n",
    "print(f\"  Only basic definition needed\")\n",
    "print(f\"  More context seems excessive\")\n",
    "print(f\"  Relevancy Score: {score_minimal_exp:.3f}\")\n",
    "\n",
    "print(f\"\\nDetailed Expected Output:\")\n",
    "print(f\"  Multiple facts needed\")\n",
    "print(f\"  Context matches expectation better\")\n",
    "print(f\"  Relevancy Score: {score_detailed_exp:.3f}\")\n",
    "\n",
    "print(f\"\\nImpact: {abs(score_minimal_exp - score_detailed_exp):.3f} difference\")\n",
    "print(\"‚ö†Ô∏è  MODERATE IMPACT - Expected Output refines relevance judgment\")\n",
    "\n",
    "# Test 4: Change ACTUAL OUTPUT (Minimal Dependency)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 4: CHANGING ACTUAL OUTPUT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "actual_good = \"Paris is the capital of France\"\n",
    "actual_bad = \"France is a country in Europe\"\n",
    "actual_none = \"\"\n",
    "\n",
    "test_good_actual = LLMTestCase(\n",
    "    input=base_query,\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=base_context,\n",
    "    actual_output=actual_good\n",
    ")\n",
    "\n",
    "test_bad_actual = LLMTestCase(\n",
    "    input=base_query,\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=base_context,\n",
    "    actual_output=actual_bad\n",
    ")\n",
    "\n",
    "test_no_actual = LLMTestCase(\n",
    "    input=base_query,\n",
    "    expected_output=base_expected,\n",
    "    retrieval_context=base_context,\n",
    "    actual_output=actual_none\n",
    ")\n",
    "\n",
    "metric.measure(test_good_actual)\n",
    "score_good_actual = metric.score\n",
    "\n",
    "metric.measure(test_bad_actual)\n",
    "score_bad_actual = metric.score\n",
    "\n",
    "metric.measure(test_no_actual)\n",
    "score_no_actual = metric.score\n",
    "\n",
    "print(f\"Good Actual Output: {score_good_actual:.3f}\")\n",
    "print(f\"Bad Actual Output:  {score_bad_actual:.3f}\")\n",
    "print(f\"No Actual Output:   {score_no_actual:.3f}\")\n",
    "\n",
    "print(f\"\\nMax Impact: {max(abs(score_good_actual - score_bad_actual), abs(score_good_actual - score_no_actual)):.3f}\")\n",
    "print(\"‚úÖ ZERO/MINIMAL IMPACT - Actual Output doesn't affect relevancy\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: DEPENDENCY RANKING FOR CONTEXTUAL RELEVANCY\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Retrieved Context  ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - PRIMARY (Content being measured)\")\n",
    "print(\"2. Input Query        ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - PRIMARY (Defines 'relevant')\")\n",
    "print(\"3. Expected Output    ‚≠ê‚≠ê‚≠ê   - SECONDARY (Refines criteria)\")\n",
    "print(\"4. Actual Output      ‚≠ê      - MINIMAL (Not used)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
